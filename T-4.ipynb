{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a1c4767",
   "metadata": {},
   "source": [
    "T4 ensembles low/mid/high models, uses pre-computed embeddings (gated for highs), and augments tails to target ~40-38 SMAPE.\n",
    "Synthetic Data: 5x replication for highs (>100, T5 paraphrasing) and lows (<10, bulk terms + quantity scaling). Validation set has no synthetic data.\n",
    "Embeddings: Load final_embeddings.pkl, map image_link to sample_id, assume 384 dims (confirm if different).\n",
    "Models: Low (LightGBM, Tweedie p=1.2), mid (V16), high (DistilBERT + embeddings, Tweedie p=1.8 + focal), DistilBERT classifier, Ridge meta-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314e76a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Feature extraction\n",
    "def extract_quantity(text):\n",
    "    match = re.search(r'(?:pack|box|set|bundle|case|dozen|carton|bulk) of (\\d+)', text, re.I)\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def extract_numeric(text, pattern):\n",
    "    match = re.search(pattern, text, re.I)\n",
    "    return float(match.group(1)) if match else 0\n",
    "\n",
    "units = {\n",
    "    'gb': r'(\\d+\\.?\\d*)\\s*gb',\n",
    "    'oz': r'(\\d+\\.?\\d*)\\s*oz',\n",
    "    'inch': r'(\\d+\\.?\\d*)\\s*(?:inch|in(?:ch)?)',\n",
    "    'mp': r'(\\d+\\.?\\d*)\\s*mp',\n",
    "    'lbs': r'(\\d+\\.?\\d*)\\s*lbs?',\n",
    "    'mah': r'(\\d+\\.?\\d*)\\s*mah',\n",
    "    'watts': r'(\\d+\\.?\\d*)\\s*w(?:atts?)?',\n",
    "    'kg': r'(\\d+\\.?\\d*)\\s*kg',\n",
    "    'ml': r'(\\d+\\.?\\d*)\\s*ml'\n",
    "}\n",
    "\n",
    "def extract_features(row):\n",
    "    text = row['catalog_content'].lower()\n",
    "    feats = {'quantity': extract_quantity(text)}\n",
    "    for unit, pattern in units.items():\n",
    "        feats[f'feat_{unit}'] = extract_numeric(text, pattern)\n",
    "    premiums = ['premium', 'luxury', 'high-end', 'pro', 'ultra', 'elite', 'deluxe', 'professional']\n",
    "    feats['premium_keyword_count'] = sum(text.count(word) for word in premiums)\n",
    "    if re.search(r'\\bnew\\b|\\bmint\\b|\\bbrand new\\b', text):\n",
    "        feats['condition_flag'] = 1\n",
    "    elif re.search(r'\\bused\\b|\\brefurbished\\b|\\bpre-owned\\b', text):\n",
    "        feats['condition_flag'] = 0\n",
    "    else:\n",
    "        feats['condition_flag'] = 0.5\n",
    "    title = re.split(r'[.:]\\s', text)[0]\n",
    "    feats['title_length'] = len(title)\n",
    "    feats['content_word_count'] = len(text.split())\n",
    "    return pd.Series(feats)\n",
    "\n",
    "# T5 paraphrasing (batched version)\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "def paraphrase_batch(batch_texts):\n",
    "    input_texts = [f\"paraphrase: {text}\" for text in batch_texts]\n",
    "    inputs = t5_tokenizer(input_texts, return_tensors='pt', max_length=128, truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = t5_model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=128, num_beams=4, early_stopping=True)\n",
    "    return [t5_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Augmentation (with batched paraphrasing)\n",
    "def augment_data(df):\n",
    "    high = df[df['price'] > 100].copy()\n",
    "    low = df[df['price'] < 10].copy()\n",
    "    # Paraphrase subset of high samples in batches\n",
    "    high_subset = high.sample(frac=0.5, random_state=42)\n",
    "    subset_texts = high_subset['catalog_content'].tolist()\n",
    "    batch_size = 4  # Adjust based on your laptop's memory; smaller if needed\n",
    "    paraphrased = []\n",
    "    for i in range(0, len(subset_texts), batch_size):\n",
    "        batch = subset_texts[i:i + batch_size]\n",
    "        paraphrased.extend(paraphrase_batch(batch))\n",
    "        gc.collect()  # Free memory after each batch\n",
    "    high.loc[high_subset.index, 'catalog_content'] = [p + ' luxury edition' for p in paraphrased]\n",
    "    low['catalog_content'] = low['catalog_content'] + ' bulk carton'\n",
    "    low['quantity'] = low['quantity'] * 2\n",
    "    high = pd.concat([high] * 5, ignore_index=True)\n",
    "    low = pd.concat([low] * 5, ignore_index=True)\n",
    "    augmented = pd.concat([df, high, low], ignore_index=True)\n",
    "    gc.collect()\n",
    "    return augmented\n",
    "\n",
    "# Load and preprocess\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "with open('input/final_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "embeddings = pd.DataFrame(embeddings).reset_index().rename(columns={'index': 'image_link'})\n",
    "embed_cols = [f'emb_{i}' for i in range(embeddings.shape[1] - 1)]\n",
    "embeddings.columns = ['image_link'] + embed_cols\n",
    "train['image_link'] = train['image_link'].astype(str)\n",
    "test['image_link'] = test['image_link'].astype(str)\n",
    "embeddings['image_link'] = embeddings['image_link'].astype(str)\n",
    "train = train.merge(embeddings, on='image_link', how='left')\n",
    "test = test.merge(embeddings, on='image_link', how='left')\n",
    "gc.collect()\n",
    "train = pd.concat([train, train.apply(extract_features, axis=1)], axis=1)\n",
    "test = pd.concat([test, test.apply(extract_features, axis=1)], axis=1)\n",
    "gc.collect()\n",
    "\n",
    "# Split (val has no synthetic data)\n",
    "X = train.drop('price', axis=1)\n",
    "y = train['price']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)\n",
    "train = X_train.assign(price=y_train)\n",
    "train = augment_data(train)\n",
    "valid = X_valid.assign(price=y_valid)\n",
    "gc.collect()\n",
    "\n",
    "num_cols = ['quantity'] + [f'feat_{u}' for u in units] + ['premium_keyword_count', 'condition_flag', 'title_length', 'content_word_count']\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler = StandardScaler()\n",
    "train[num_cols] = train[num_cols].astype(np.float32)\n",
    "valid[num_cols] = valid[num_cols].astype(np.float32)\n",
    "test[num_cols] = test[num_cols].astype(np.float32)\n",
    "train[num_cols] = imputer.fit_transform(train[num_cols])\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "valid[num_cols] = imputer.transform(valid[num_cols])\n",
    "valid[num_cols] = scaler.transform(valid[num_cols])\n",
    "test[num_cols] = imputer.transform(test[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "train[embed_cols] = train[embed_cols].astype(np.float32)\n",
    "valid[embed_cols] = valid[embed_cols].astype(np.float32)\n",
    "test[embed_cols] = test[embed_cols].astype(np.float32)\n",
    "gc.collect()\n",
    "\n",
    "# Classifier Dataset\n",
    "class ClassifierDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df['catalog_content'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.labels = (df['price'] > 100).astype(int).values if 'price' in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# High-Price Dataset\n",
    "class HighPriceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df['catalog_content'].values\n",
    "        self.numerics = df[num_cols].values.astype(np.float32)\n",
    "        self.embeds = df[embed_cols].values.astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prices = df['price'].values if 'price' in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float32),\n",
    "            'embeds': torch.tensor(self.embeds[idx], dtype=torch.float32)\n",
    "        }\n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# Models\n",
    "class TweedieLoss(nn.Module):\n",
    "    def __init__(self, p=1.8):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        term1 = -target * torch.pow(pred, 1 - self.p) / (1 - self.p)\n",
    "        term2 = torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
    "        return torch.mean(term1 + term2)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        l = torch.abs(pred - target)\n",
    "        return torch.mean(l * torch.pow(l, self.gamma))\n",
    "\n",
    "class HighPriceRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.fc1 = nn.Linear(768 + len(num_cols) + len(embed_cols), 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics, embeds):\n",
    "        text_emb = self.text_model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat([text_emb, numerics, embeds], dim=1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Preprocessing for LightGBM\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1, 2), max_features=20000)  # Reduced\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'catalog_content'),\n",
    "        ('num', num_transformer, num_cols)\n",
    "    ])\n",
    "\n",
    "# Train Classifier\n",
    "device = torch.device('cpu')  # M1 Pro, no GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_clf_ds = ClassifierDataset(train, tokenizer)\n",
    "valid_clf_ds = ClassifierDataset(valid, tokenizer)\n",
    "train_clf_loader = DataLoader(train_clf_ds, batch_size=2, shuffle=True)  # Reduced batch size\n",
    "valid_clf_loader = DataLoader(valid_clf_ds, batch_size=2)\n",
    "clf_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "clf_fc = nn.Linear(768, 2).to(device)\n",
    "optimizer = torch.optim.AdamW(list(clf_model.parameters()) + list(clf_fc.parameters()), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(2):\n",
    "    clf_model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_clf_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].to(device)\n",
    "        emb = clf_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = clf_fc(emb)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    clf_model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_clf_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].to(device)\n",
    "            emb = clf_model(**inputs).last_hidden_state[:, 0, :]\n",
    "            logits = clf_fc(emb)\n",
    "            val_loss += loss_fn(logits, labels).item()\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(clf_fc.state_dict(), 'clf_fc.pt')\n",
    "    gc.collect()\n",
    "\n",
    "# Train Low/Mid Models\n",
    "X_train_lgb = preprocessor.fit_transform(train)\n",
    "X_valid_lgb = preprocessor.transform(valid)\n",
    "X_test_lgb = preprocessor.transform(test)\n",
    "gc.collect()\n",
    "y_train_lgb = train['price']\n",
    "y_valid_lgb = valid['price']\n",
    "low_mask = y_train_lgb < 10\n",
    "mid_mask = (y_train_lgb >= 10) & (y_train_lgb <= 100)\n",
    "params_low = {'objective': 'tweedie', 'tweedie_variance_power': 1.2, 'learning_rate': 0.08, 'num_leaves': 31, 'min_data_in_leaf': 20, 'feature_pre_filter': False, 'verbose': -1}\n",
    "params_mid = {'objective': 'regression', 'learning_rate': 0.05, 'num_leaves': 31, 'min_data_in_leaf': 20, 'feature_pre_filter': False, 'verbose': -1}\n",
    "dtrain_low = lgb.Dataset(X_train_lgb[low_mask], label=np.log1p(y_train_lgb[low_mask]))\n",
    "dtrain_mid = lgb.Dataset(X_train_lgb[mid_mask], label=np.log1p(y_train_lgb[mid_mask]))\n",
    "dvalid = lgb.Dataset(X_valid_lgb, label=np.log1p(y_valid_lgb))\n",
    "model_low = lgb.train(params_low, dtrain_low, num_boost_round=1000, valid_sets=[dvalid], callbacks=[lgb.early_stopping(50)])\n",
    "model_mid = lgb.train(params_mid, dtrain_mid, num_boost_round=1000, valid_sets=[dvalid], callbacks=[lgb.early_stopping(50)])\n",
    "gc.collect()\n",
    "\n",
    "# Train High Model\n",
    "train_high_ds = HighPriceDataset(train[train['price'] > 100], tokenizer)\n",
    "valid_high_ds = HighPriceDataset(valid[valid['price'] > 100], tokenizer)\n",
    "train_high_loader = DataLoader(train_high_ds, batch_size=2, shuffle=True)  # Reduced batch size\n",
    "valid_high_loader = DataLoader(valid_high_ds, batch_size=2)\n",
    "high_model = HighPriceRegressor().to(device)\n",
    "optimizer = torch.optim.AdamW(high_model.parameters(), lr=2e-5)\n",
    "tweedie_loss = TweedieLoss(p=1.8)\n",
    "focal_loss = FocalLoss(gamma=1)\n",
    "best_val_smape = float('inf')\n",
    "for epoch in range(2):\n",
    "    high_model.train()\n",
    "    for batch in train_high_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "        target = batch['price'].to(device)\n",
    "        pred = high_model(**inputs).squeeze()\n",
    "        loss = 0.7 * tweedie_loss(pred, target) + 0.3 * focal_loss(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    high_model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_high_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "            target = batch['price'].to(device)\n",
    "            pred = high_model(**inputs).squeeze()\n",
    "            preds.extend(np.expm1(pred.cpu().numpy()))\n",
    "            actuals.extend(np.expm1(target.cpu().numpy()))\n",
    "    val_smape = smape(np.array(actuals), np.array(preds))\n",
    "    if val_smape < best_val_smape:\n",
    "        best_val_smape = val_smape\n",
    "        torch.save(high_model.state_dict(), 'high_model.pt')\n",
    "    gc.collect()\n",
    "\n",
    "# Predict\n",
    "clf_model.eval()\n",
    "clf_fc.load_state_dict(torch.load('clf_fc.pt'))\n",
    "high_model.load_state_dict(torch.load('high_model.pt'))\n",
    "pred_low = np.expm1(model_low.predict(X_valid_lgb))\n",
    "pred_mid = np.expm1(model_mid.predict(X_valid_lgb))\n",
    "pred_high = []\n",
    "valid_high_loader = DataLoader(HighPriceDataset(valid[valid['price'] > 100], tokenizer), batch_size=2)  # Reduced\n",
    "with torch.no_grad():\n",
    "    for batch in valid_high_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "        pred = high_model(**inputs).squeeze()\n",
    "        pred_high.extend(np.expm1(pred.cpu().numpy()))\n",
    "pred_high = np.array(pred_high)\n",
    "valid_clf_ds = ClassifierDataset(valid, tokenizer)\n",
    "valid_clf_loader = DataLoader(valid_clf_ds, batch_size=2)  # Reduced\n",
    "is_high = []\n",
    "with torch.no_grad():\n",
    "    for batch in valid_clf_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        emb = clf_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = clf_fc(emb)\n",
    "        is_high.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy() > 0.5)\n",
    "is_high = np.array(is_high)\n",
    "is_low = valid['price'] < 10\n",
    "is_mid = (~is_high) & (~is_low)\n",
    "if len(pred_high) < is_high.sum():\n",
    "    pred_high = np.pad(pred_high, (0, is_high.sum() - len(pred_high)), constant_values=pred_mid[:is_high.sum() - len(pred_high)])\n",
    "elif len(pred_high) > is_high.sum():\n",
    "    pred_high = pred_high[:is_high.sum()]\n",
    "preds = np.where(is_high, pred_high, np.where(is_low, pred_low, pred_mid))\n",
    "smape_score = smape(valid['price'], preds)\n",
    "print(f'T4 Validation SMAPE: {smape_score}')\n",
    "gc.collect()\n",
    "\n",
    "# Error Analysis\n",
    "errors = pd.DataFrame({\n",
    "    'actual': valid['price'],\n",
    "    'pred': preds,\n",
    "    'diff': preds - valid['price'],\n",
    "    'ape': 2 * 100 * np.abs(preds - valid['price']) / (np.abs(valid['price']) + np.abs(preds))\n",
    "})\n",
    "bins = [0, 10, 50, 100, 500, np.inf]\n",
    "labels = ['0-10 (Low/Bulk)', '10-50', '50-100', '100-500 (High)', '500+ (Extreme)']\n",
    "errors['price_bin'] = pd.cut(errors['actual'], bins=bins, labels=labels)\n",
    "bin_smape = errors.groupby('price_bin')['ape'].mean() / 2\n",
    "print('SMAPE per bin:\\n', bin_smape)\n",
    "bin_bias = errors.groupby('price_bin')['diff'].mean()\n",
    "print('Bias per bin:\\n', bin_bias)\n",
    "bin_var = errors.groupby('price_bin')['diff'].var()\n",
    "print('Error variance per bin:\\n', bin_var)\n",
    "top_errors = errors.sort_values('ape', ascending=False).head(10)\n",
    "print('Top 10 worst predictions:\\n', top_errors)\n",
    "gc.collect()\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = f'output/T4_{timestamp}'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.save(high_model.state_dict(), os.path.join(out_dir, 'high_model.pt'))\n",
    "model_low.save_model(os.path.join(out_dir, 'low_model.txt'))\n",
    "model_mid.save_model(os.path.join(out_dir, 'mid_model.txt'))\n",
    "pd.DataFrame({'actual': valid['price'], 'pred': preds}).to_csv(os.path.join(out_dir, 'preds_valid.csv'), index=False)\n",
    "with open(os.path.join(out_dir, 'error_analysis.txt'), 'w') as f:\n",
    "    f.write(f'Validation SMAPE: {smape_score}\\n')\n",
    "    f.write('SMAPE per bin:\\n' + str(bin_smape) + '\\n')\n",
    "    f.write('Bias per bin:\\n' + str(bin_bias) + '\\n')\n",
    "    f.write('Error variance per bin:\\n' + str(bin_var) + '\\n')\n",
    "    f.write('Top 10 worst:\\n' + str(top_errors) + '\\n')\n",
    "\n",
    "# Test inference\n",
    "test_clf_ds = ClassifierDataset(test, tokenizer)\n",
    "test_clf_loader = DataLoader(test_clf_ds, batch_size=2)  # Reduced\n",
    "is_high_test = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_clf_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        emb = clf_model(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = clf_fc(emb)\n",
    "        is_high_test.extend(torch.softmax(logits, dim=1)[:, 1].cpu().numpy() > 0.5)\n",
    "is_high_test = np.array(is_high_test)\n",
    "pred_low = np.expm1(model_low.predict(X_test_lgb))\n",
    "pred_mid = np.expm1(model_mid.predict(X_test_lgb))\n",
    "test_high_ds = HighPriceDataset(test[is_high_test], tokenizer)\n",
    "test_high_loader = DataLoader(test_high_ds, batch_size=2)  # Reduced\n",
    "pred_high = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_high_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = high_model(**inputs).squeeze()\n",
    "        pred_high.extend(np.expm1(pred.cpu().numpy()))\n",
    "pred_high = np.array(pred_high)\n",
    "if len(pred_high) < is_high_test.sum():\n",
    "    pred_high = np.pad(pred_high, (0, is_high_test.sum() - len(pred_high)), constant_values=pred_mid[:is_high_test.sum() - len(pred_high)])\n",
    "elif len(pred_high) > is_high_test.sum():\n",
    "    pred_high = pred_high[:is_high_test.sum()]\n",
    "pred_test = np.where(is_high_test, pred_high, pred_mid)\n",
    "submission = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test})\n",
    "submission.to_csv(os.path.join(out_dir, 'submission.csv'), index=False)\n",
    "print(f'Outputs saved to {out_dir}')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781891d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (housing-price-prediction)",
   "language": "python",
   "name": "housing-price-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
