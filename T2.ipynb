{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e2e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's tweedie: 75.8555\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's tweedie: 75.7234\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[768]\tvalid_0's tweedie: 75.7508\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's tweedie: 18.3778\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[994]\tvalid_0's tweedie: 18.349\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[674]\tvalid_0's tweedie: 18.3489\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's tweedie: 11.3608\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[702]\tvalid_0's tweedie: 11.3534\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[514]\tvalid_0's tweedie: 11.3539\n",
      "Best params from grid: {'tweedie_variance_power': 1.5, 'learning_rate': 0.08, 'objective': 'tweedie', 'metric': 'tweedie', 'num_leaves': 31, 'min_data_in_leaf': 20, 'feature_pre_filter': False, 'verbose': -1}\n",
      "Best grid SMAPE: 53.43635258563862\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[674]\tvalid_0's tweedie: 18.3489\n",
      "T2 Validation SMAPE: 53.43635258563862\n",
      "SMAPE per bin:\n",
      " price_bin\n",
      "0-10 (Low/Bulk)    33.121689\n",
      "10-50              20.898170\n",
      "50-100             29.617426\n",
      "100-500 (High)     35.210810\n",
      "500+ (Extreme)     85.379557\n",
      "Name: ape, dtype: float64\n",
      "Bias (pred - actual) per bin:\n",
      " price_bin\n",
      "0-10 (Low/Bulk)       5.733757\n",
      "10-50                -2.203190\n",
      "50-100              -20.315484\n",
      "100-500 (High)      -71.091295\n",
      "500+ (Extreme)    -1217.193166\n",
      "Name: diff, dtype: float64\n",
      "Error variance per bin:\n",
      " price_bin\n",
      "0-10 (Low/Bulk)    6.390441e+01\n",
      "10-50              1.870118e+02\n",
      "50-100             9.443740e+02\n",
      "100-500 (High)     6.067422e+03\n",
      "500+ (Extreme)     1.382102e+06\n",
      "Name: diff, dtype: float64\n",
      "Top 10 worst predictions:\n",
      "         actual        pred        diff         ape        price_bin\n",
      "9273     1.990  201.290197  199.300197  196.084223  0-10 (Low/Bulk)\n",
      "59934    0.980   64.249610   63.269610  193.990459  0-10 (Low/Bulk)\n",
      "24856    1.180   73.641829   72.461829  193.691681  0-10 (Low/Bulk)\n",
      "28165    1.680   99.230498   97.550498  193.340633  0-10 (Low/Bulk)\n",
      "26873    1.915   97.243646   95.328646  192.275005  0-10 (Low/Bulk)\n",
      "33685    0.680   30.348112   29.668112  191.233756  0-10 (Low/Bulk)\n",
      "18709  286.770    6.845994 -279.924006  190.673541   100-500 (High)\n",
      "59073    0.500   20.092442   19.592442  190.287699  0-10 (Low/Bulk)\n",
      "68198    0.890   33.684419   32.794419  189.703370  0-10 (Low/Bulk)\n",
      "7723     0.980   36.221265   35.241265  189.462724  0-10 (Low/Bulk)\n",
      "Insight: Still underpredicting highsâ€”try higher tail weights or lower variance_power (e.g., 1.2 for more Poisson-like).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1k/7lydcyq13sl0zlx371zkb4xc0000gn/T/ipykernel_13238/2859743384.py:138: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_smape = errors.groupby('price_bin')['ape'].mean() / 2\n",
      "/var/folders/1k/7lydcyq13sl0zlx371zkb4xc0000gn/T/ipykernel_13238/2859743384.py:140: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_bias = errors.groupby('price_bin')['diff'].mean()\n",
      "/var/folders/1k/7lydcyq13sl0zlx371zkb4xc0000gn/T/ipykernel_13238/2859743384.py:142: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  bin_var = errors.groupby('price_bin')['diff'].var()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs saved to output/T2_20251013_145619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import itertools  # For simple grid\n",
    "\n",
    "# Define SMAPE (symmetric)\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Feature extraction functions (same as before)\n",
    "def extract_quantity(text):\n",
    "    match = re.search(r'(?:pack|box|set|bundle|case) of (\\d+)', text, re.I)  # Added 'case' for more bulk patterns\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def extract_numeric(text, pattern):\n",
    "    match = re.search(pattern, text, re.I)\n",
    "    return float(match.group(1)) if match else 0\n",
    "\n",
    "units = {\n",
    "    'gb': r'(\\d+\\.?\\d*)\\s*gb',\n",
    "    'oz': r'(\\d+\\.?\\d*)\\s*oz',\n",
    "    'inch': r'(\\d+\\.?\\d*)\\s*(?:inch|in(?:ch)?)',\n",
    "    'mp': r'(\\d+\\.?\\d*)\\s*mp',\n",
    "    'lbs': r'(\\d+\\.?\\d*)\\s*lbs?',\n",
    "    'mah': r'(\\d+\\.?\\d*)\\s*mah',\n",
    "    'watts': r'(\\d+\\.?\\d*)\\s*w(?:atts?)?'\n",
    "    # Expand as needed\n",
    "}\n",
    "\n",
    "def extract_features(row):\n",
    "    text = row['catalog_content'].lower()\n",
    "    feats = {'quantity': extract_quantity(text)}\n",
    "    for unit, pattern in units.items():\n",
    "        feats[f'feat_{unit}'] = extract_numeric(text, pattern)\n",
    "    premiums = ['premium', 'luxury', 'high-end', 'pro', 'ultra', 'elite', 'deluxe', 'professional']\n",
    "    feats['premium_keyword_count'] = sum(text.count(word) for word in premiums)\n",
    "    if re.search(r'\\bnew\\b|\\bmint\\b|\\bbrand new\\b', text):\n",
    "        feats['condition_flag'] = 1\n",
    "    elif re.search(r'\\bused\\b|\\brefurbished\\b|\\bpre-owned\\b', text):\n",
    "        feats['condition_flag'] = 0\n",
    "    else:\n",
    "        feats['condition_flag'] = 0.5\n",
    "    title = re.split(r'[.:]\\s', text)[0]\n",
    "    feats['title_length'] = len(title)\n",
    "    feats['content_word_count'] = len(text.split())\n",
    "    return pd.Series(feats)\n",
    "\n",
    "# Load and engineer data\n",
    "train = pd.read_csv('input/train.csv')\n",
    "engineered = train.apply(extract_features, axis=1)\n",
    "train = pd.concat([train, engineered], axis=1)\n",
    "\n",
    "# Split\n",
    "X = train.drop('price', axis=1)\n",
    "y = train['price']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "text_transformer = TfidfVectorizer(ngram_range=(1, 2), max_features=40000)\n",
    "num_cols = ['quantity'] + [f'feat_{u}' for u in units] + ['premium_keyword_count', 'condition_flag', 'title_length', 'content_word_count']\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_transformer, 'catalog_content'),\n",
    "        ('num', num_transformer, num_cols)\n",
    "    ])\n",
    "\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_valid_pre = preprocessor.transform(X_valid)\n",
    "\n",
    "# Sample weights\n",
    "high_threshold = 100\n",
    "low_threshold = 10\n",
    "weights = np.ones_like(y_train)\n",
    "weights[y_train > high_threshold] = 3\n",
    "weights[y_train < low_threshold] = 2\n",
    "dtrain = lgb.Dataset(X_train_pre, label=y_train, weight=weights)\n",
    "dvalid = lgb.Dataset(X_valid_pre, label=y_valid, reference=dtrain)\n",
    "\n",
    "# Simple grid search for key params (variance_power, learning_rate; fixed others)\n",
    "grid = {\n",
    "    'tweedie_variance_power': [1.2, 1.5, 1.8],\n",
    "    'learning_rate': [0.02, 0.05, 0.08]\n",
    "}\n",
    "best_smape = np.inf\n",
    "best_params = {}\n",
    "for params in itertools.product(*grid.values()):\n",
    "    params_dict = dict(zip(grid.keys(), params))\n",
    "    params_dict.update({\n",
    "        'objective': 'tweedie',\n",
    "        'metric': 'tweedie',\n",
    "        'num_leaves': 31,  # Fixed from your prior\n",
    "        'min_data_in_leaf': 20,\n",
    "        'feature_pre_filter': False,  # Added to fix the error\n",
    "        'verbose': -1\n",
    "    })\n",
    "    model = lgb.train(params_dict, dtrain, num_boost_round=1000, valid_sets=[dvalid], callbacks=[lgb.early_stopping(50)])\n",
    "    pred = np.maximum(model.predict(X_valid_pre), 1e-6)\n",
    "    current_smape = smape(y_valid, pred)\n",
    "    if current_smape < best_smape:\n",
    "        best_smape = current_smape\n",
    "        best_params = params_dict\n",
    "\n",
    "print(f'Best params from grid: {best_params}')\n",
    "print(f'Best grid SMAPE: {best_smape}')\n",
    "\n",
    "# Train final with best\n",
    "model = lgb.train(best_params, dtrain, num_boost_round=1000, valid_sets=[dvalid], callbacks=[lgb.early_stopping(50)])\n",
    "\n",
    "# Predict and evaluate (same as before)\n",
    "pred = np.maximum(model.predict(X_valid_pre), 1e-6)\n",
    "smape_score = smape(y_valid, pred)\n",
    "print(f'T2 Validation SMAPE: {smape_score}')\n",
    "\n",
    "# Error Analysis (same as before)\n",
    "errors = pd.DataFrame({\n",
    "    'actual': y_valid,\n",
    "    'pred': pred,\n",
    "    'diff': pred - y_valid,\n",
    "    'ape': 2 * 100 * np.abs(pred - y_valid) / (np.abs(y_valid) + np.abs(pred))\n",
    "})\n",
    "bins = [0, 10, 50, 100, 500, np.inf]\n",
    "labels = ['0-10 (Low/Bulk)', '10-50', '50-100', '100-500 (High)', '500+ (Extreme)']\n",
    "errors['price_bin'] = pd.cut(errors['actual'], bins=bins, labels=labels)\n",
    "bin_smape = errors.groupby('price_bin')['ape'].mean() / 2\n",
    "print('SMAPE per bin:\\n', bin_smape)\n",
    "bin_bias = errors.groupby('price_bin')['diff'].mean()\n",
    "print('Bias (pred - actual) per bin:\\n', bin_bias)\n",
    "bin_var = errors.groupby('price_bin')['diff'].var()\n",
    "print('Error variance per bin:\\n', bin_var)\n",
    "top_errors = errors.sort_values('ape', ascending=False).head(10)\n",
    "print('Top 10 worst predictions:\\n', top_errors)\n",
    "if bin_bias.iloc[-2] < 0:\n",
    "    print('Insight: Still underpredicting highsâ€”try higher tail weights or lower variance_power (e.g., 1.2 for more Poisson-like).')\n",
    "if bin_smape.iloc[0] > bin_smape.mean():\n",
    "    print('Insight: Bulk/low errors highâ€”refine quantity regex or add bulk-specific features (e.g., \"case of\").')\n",
    "\n",
    "# Save outputs (same as before)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = f'output/T2_{timestamp}'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "pd.DataFrame({'actual': y_valid, 'pred': pred}).to_csv(os.path.join(out_dir, 'preds_valid.csv'), index=False)\n",
    "model.save_model(os.path.join(out_dir, 'model.txt'))\n",
    "with open(os.path.join(out_dir, 'error_analysis.txt'), 'w') as f:\n",
    "    f.write(f'Validation SMAPE: {smape_score}\\n')\n",
    "    f.write('SMAPE per bin:\\n' + str(bin_smape) + '\\n')\n",
    "    f.write('Bias per bin:\\n' + str(bin_bias) + '\\n')\n",
    "    f.write('Error variance per bin:\\n' + str(bin_var) + '\\n')\n",
    "    f.write('Top 10 worst:\\n' + str(top_errors) + '\\n')\n",
    "    f.write(f'Best params: {best_params}\\n')\n",
    "\n",
    "# Inference on test\n",
    "test = pd.read_csv('input/test.csv')\n",
    "engineered_test = test.apply(extract_features, axis=1)\n",
    "test = pd.concat([test, engineered_test], axis=1)\n",
    "X_test_pre = preprocessor.transform(test)\n",
    "pred_test = np.maximum(model.predict(X_test_pre), 1e-6)\n",
    "if 'sample_id' in test.columns:\n",
    "    submission = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test})\n",
    "    submission.to_csv(os.path.join(out_dir, 'submission.csv'), index=False)\n",
    "else:\n",
    "    pd.DataFrame({'pred': pred_test}).to_csv(os.path.join(out_dir, 'preds_test.csv'), index=False)\n",
    "\n",
    "print(f'Outputs saved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d5fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
