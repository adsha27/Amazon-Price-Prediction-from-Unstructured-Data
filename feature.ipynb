{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60ebca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded /Users/adityasharma/Github Projects/Amazon/input/train.csv\n",
      "Dataset shape: (75000, 4)\n",
      "\n",
      "--- 1. Structural Characterization ---\n",
      "\n",
      "Distribution of Line Counts per Entry:\n",
      "count    75000.000000\n",
      "mean         8.002773\n",
      "std          2.340287\n",
      "min          4.000000\n",
      "25%          6.000000\n",
      "50%          9.000000\n",
      "75%         10.000000\n",
      "max         31.000000\n",
      "Name: line_count, dtype: float64\n",
      "\n",
      "Distribution of Character Counts per Entry:\n",
      "count    75000.000000\n",
      "mean       908.886547\n",
      "std        852.896151\n",
      "min         32.000000\n",
      "25%        251.000000\n",
      "50%        643.000000\n",
      "75%       1280.000000\n",
      "max       7894.000000\n",
      "Name: char_count, dtype: float64\n",
      "\n",
      "--- 2. Key-Value Pair Analysis ---\n",
      "\n",
      "Top 20 Most Common Keys Found:\n",
      "- 'item name': 74994 occurrences\n",
      "- '0\n",
      "unit': 55005 occurrences\n",
      "- 'value': 36582 occurrences\n",
      "- 'bullet point 2': 25017 occurrences\n",
      "- 'bullet point 3': 23910 occurrences\n",
      "- 'bullet point 1': 20022 occurrences\n",
      "- 'bullet point 4': 19788 occurrences\n",
      "- 'bullet point 5': 17994 occurrences\n",
      "- 'product description': 17283 occurrences\n",
      "- 'ingredients': 6716 occurrences\n",
      "- 'zin': 3981 occurrences\n",
      "- 'size': 3690 occurrences\n",
      "- 'manufacturer': 3414 occurrences\n",
      "- '5\n",
      "unit': 3147 occurrences\n",
      "- 'bullet point': 2966 occurrences\n",
      "- 'terravita\n",
      "bullet point 4': 2892 occurrences\n",
      "- 'bullet point 6': 2574 occurrences\n",
      "- 'hot tea brewing method': 2480 occurrences\n",
      "- 'iced tea brewing method': 2418 occurrences\n",
      "- '8\n",
      "unit': 1786 occurrences\n",
      "\n",
      "--- 3. Coverage Analysis ---\n",
      "\n",
      "Coverage: 100.00% of entries contain at least one of the top 20 keys.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Use the exact file path provided ---\n",
    "file_path = '/Users/adityasharma/Github Projects/Amazon/input/train.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded {file_path}\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at the specified path: {file_path}\")\n",
    "    print(\"Halting analysis. Please verify the path is correct.\")\n",
    "    exit()\n",
    "\n",
    "# Ensure the content column is of string type, filling NaNs to prevent errors\n",
    "df['catalog_content'] = df['catalog_content'].astype(str).fillna('')\n",
    "\n",
    "# --- Step 1: High-Level Structural Characterization ---\n",
    "print(\"\\n--- 1. Structural Characterization ---\")\n",
    "df['line_count'] = df['catalog_content'].str.count('\\n') + 1\n",
    "df['char_count'] = df['catalog_content'].str.len()\n",
    "\n",
    "print(\"\\nDistribution of Line Counts per Entry:\")\n",
    "print(df['line_count'].describe())\n",
    "\n",
    "print(\"\\nDistribution of Character Counts per Entry:\")\n",
    "print(df['char_count'].describe())\n",
    "\n",
    "# --- Step 2: Key-Value Pair Analysis ---\n",
    "print(\"\\n--- 2. Key-Value Pair Analysis ---\")\n",
    "\n",
    "# Regex to find potential keys (alphanumeric sequences before a colon)\n",
    "key_pattern = re.compile(r'([a-zA-Z0-9\\s/]+):')\n",
    "\n",
    "def find_keys(text):\n",
    "    # Find all potential keys, clean them up, and convert to lowercase\n",
    "    return [key.strip().lower() for key in key_pattern.findall(text)]\n",
    "\n",
    "# Apply the function and create a flat list of all keys found\n",
    "all_keys = df['catalog_content'].apply(find_keys).sum()\n",
    "\n",
    "# Count the frequency of each unique key\n",
    "key_counts = Counter(all_keys)\n",
    "\n",
    "print(\"\\nTop 20 Most Common Keys Found:\")\n",
    "for key, count in key_counts.most_common(20):\n",
    "    print(f\"- '{key}': {count} occurrences\")\n",
    "\n",
    "# --- Step 3: Coverage Analysis ---\n",
    "print(\"\\n--- 3. Coverage Analysis ---\")\n",
    "total_entries = len(df)\n",
    "if total_entries > 0:\n",
    "    top_20_keys = {k for k, v in key_counts.most_common(20)}\n",
    "    \n",
    "    def has_top_key(text):\n",
    "        # A more robust check for any of the top keys\n",
    "        return any(re.search(fr'\\b{re.escape(k)}:', text, re.IGNORECASE) for k in top_20_keys)\n",
    "\n",
    "    df['has_top_key'] = df['catalog_content'].apply(has_top_key)\n",
    "    coverage = df['has_top_key'].sum() / total_entries\n",
    "    print(f\"\\nCoverage: {coverage:.2%} of entries contain at least one of the top 20 keys.\")\n",
    "else:\n",
    "    print(\"Dataset is empty. Cannot perform coverage analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa1e72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analysis of ROBUST 'title' column extraction ---\n",
      "- Extracted successfully in 74993 / 75000 rows (99.99%)\n",
      "\n",
      "--- Analysis of 'aggregated_bullets' column ---\n",
      "- Extracted successfully in 60723 / 75000 rows (80.96%)\n",
      "\n",
      "--- Example: No entry with 'bullet point 6' found for verification. ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Assume 'df' is the DataFrame loaded in the previous steps.\n",
    "# file_path = '/Users/adityasharma/Github Projects/Amazon/input/train.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "# df['catalog_content'] = df['catalog_content'].astype(str).fillna('')\n",
    "\n",
    "def robust_extract_field(text, key):\n",
    "    \"\"\"\n",
    "    A robust function to extract a value, even if it spans multiple lines.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(fr'^{re.escape(key)}:\\s*(.*?)(?=\\n[a-zA-Z0-9\\s/]+:|\\Z)', re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return \" \".join(match.group(1).strip().split())\n",
    "    return None\n",
    "\n",
    "def aggregate_bullet_points(text):\n",
    "    \"\"\"\n",
    "    Finds all lines starting with 'bullet point', extracts the text, and joins them.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'^(?:bullet point|bullet point \\d+):\\s*(.*)', re.IGNORECASE | re.MULTILINE)\n",
    "    matches = pattern.findall(text)\n",
    "    if matches:\n",
    "        return ' '.join([match.strip() for match in matches])\n",
    "    return None\n",
    "\n",
    "# --- Re-run the extraction with the ROBUST function ---\n",
    "df['title'] = df['catalog_content'].apply(lambda x: robust_extract_field(x, 'item name'))\n",
    "df['aggregated_bullets'] = df['catalog_content'].apply(aggregate_bullet_points)\n",
    "\n",
    "\n",
    "# --- Re-run the analysis to verify the new function's success rate ---\n",
    "title_nulls = df['title'].isnull().sum()\n",
    "bullets_nulls = df['aggregated_bullets'].isnull().sum()\n",
    "total_rows = len(df)\n",
    "\n",
    "print(\"--- Analysis of ROBUST 'title' column extraction ---\")\n",
    "print(f\"- Extracted successfully in {total_rows - title_nulls} / {total_rows} rows ({(total_rows - title_nulls) / total_rows:.2%})\")\n",
    "\n",
    "print(f\"\\n--- Analysis of 'aggregated_bullets' column ---\")\n",
    "print(f\"- Extracted successfully in {total_rows - bullets_nulls} / {total_rows} rows ({(total_rows - bullets_nulls) / total_rows:.2%})\")\n",
    "\n",
    "# --- CORRECTED VERIFICATION STEP ---\n",
    "# Search for a more reliable key that we know exists from our analysis.\n",
    "complex_entries_df = df[df['catalog_content'].str.contains(\"bullet point 6\", na=False)]\n",
    "\n",
    "# First, check if the search returned any results before trying to access an index.\n",
    "if not complex_entries_df.empty:\n",
    "    complex_entry = complex_entries_df.iloc[0]\n",
    "    print(\"\\n--- Example: Verification of a complex entry ---\")\n",
    "    # Display enough text to see the structure.\n",
    "    print(\"Original Content Snippet:\\n\", complex_entry['catalog_content'][0:700])\n",
    "    print(\"\\nExtracted Title:\\n\", complex_entry['title'])\n",
    "    print(\"\\nExtracted Bullets:\\n\", complex_entry['aggregated_bullets'])\n",
    "else:\n",
    "    print(\"\\n--- Example: No entry with 'bullet point 6' found for verification. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53483a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 60000 samples, validating on 15000 samples.\n",
      "\n",
      "Training the TF-IDF + Ridge pipeline on the 'title' column...\n",
      "Training complete.\n",
      "\n",
      "Making predictions on the validation set...\n",
      "\n",
      "--- Performance Evaluation ---\n",
      "Original Baseline SMAPE (on catalog_content): 56.35\n",
      "New Baseline SMAPE (on extracted 'title'): 69.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We assume 'df' is the DataFrame with the new 'title' and 'price' columns.\n",
    "# To ensure we have the 'price' column, we need to reload the original train.csv\n",
    "# and merge our new features onto it.\n",
    "\n",
    "# Load original data to get the 'price' target variable\n",
    "original_train_df = pd.read_csv('/Users/adityasharma/Github Projects/Amazon/input/train.csv')\n",
    "\n",
    "# Use the 'df' from the previous step which already has 'title' and 'aggregated_bullets'\n",
    "# Make sure the 'price' column is merged correctly. The dataframes should have the same index.\n",
    "df['price'] = original_train_df['price']\n",
    "\n",
    "# --- SMAPE Metric Definition ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# We will use a proper validation split to measure performance.\n",
    "# Handle the few (<0.01%) titles that were not extracted by filling with an empty string.\n",
    "df['title'] = df['title'].fillna('')\n",
    "train_df = df.dropna(subset=['price']) # Ensure we only use rows with a price for training\n",
    "\n",
    "X = train_df['title']\n",
    "y = train_df['price']\n",
    "\n",
    "# Create a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- Model Training ---\n",
    "# Create the exact same pipeline as the original baseline.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english', max_features=20000),\n",
    "    Ridge(alpha=1.0)\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the TF-IDF + Ridge pipeline on the 'title' column...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nMaking predictions on the validation set...\")\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Prevent division by zero or negative price predictions\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "validation_smape = smape(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Performance Evaluation ---\")\n",
    "print(f\"Original Baseline SMAPE (on catalog_content): 56.35\")\n",
    "print(f\"New Baseline SMAPE (on extracted 'title'): {validation_smape:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0943faf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 60000 samples, validating on 15000 samples.\n",
      "\n",
      "Training a multi-input pipeline on 'title' and 'aggregated_bullets'...\n",
      "Training complete.\n",
      "\n",
      "Making predictions on the validation set...\n",
      "\n",
      "--- Performance Evaluation ---\n",
      "Original Baseline SMAPE (on catalog_content): 56.35\n",
      "Second Baseline SMAPE (on title only): 69.92\n",
      "New Model SMAPE (on title + bullets): 73.79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We continue with the 'df' from the previous step, which has 'title', \n",
    "# 'aggregated_bullets', and 'price' columns.\n",
    "\n",
    "# --- SMAPE Metric Definition ---\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Fill any remaining NaNs in our text features\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['aggregated_bullets'] = df['aggregated_bullets'].fillna('')\n",
    "train_df = df.dropna(subset=['price'])\n",
    "\n",
    "# Here, X is a DataFrame with two columns\n",
    "X = train_df[['title', 'aggregated_bullets']]\n",
    "y = train_df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training on {len(X_train)} samples, validating on {len(X_val)} samples.\")\n",
    "\n",
    "# --- Multi-Input Model Pipeline ---\n",
    "# We define a preprocessor that applies a separate TF-IDF to each column.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('title_tfidf', TfidfVectorizer(stop_words='english', max_features=10000), 'title'),\n",
    "        ('bullets_tfidf', TfidfVectorizer(stop_words='english', max_features=15000), 'aggregated_bullets')\n",
    "    ],\n",
    "    remainder='drop' # Drop any other columns\n",
    ")\n",
    "\n",
    "# Chain the preprocessor and the regressor in a final pipeline.\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "print(\"\\nTraining a multi-input pipeline on 'title' and 'aggregated_bullets'...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nMaking predictions on the validation set...\")\n",
    "y_pred = pipeline.predict(X_val)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "validation_smape = smape(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Performance Evaluation ---\")\n",
    "print(f\"Original Baseline SMAPE (on catalog_content): 56.35\")\n",
    "print(f\"Second Baseline SMAPE (on title only): 69.92\")\n",
    "print(f\"New Model SMAPE (on title + bullets): {validation_smape:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a7240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Hybrid Feature Pipeline...\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "--- Final Performance Evaluation ---\n",
      "Original Baseline SMAPE (raw text): 56.35\n",
      "Hybrid Model SMAPE (structured features): 70.22\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# --- Assume 'df' is loaded and has the 'price' column ---\n",
    "# We'll use the robust extraction function from before\n",
    "def robust_extract_field(text, key):\n",
    "    pattern = re.compile(fr'^{re.escape(key)}:\\s*(.*?)(?=\\n[a-zA-Z0-9\\s/]+:|\\Z)', re.IGNORECASE | re.MULTILINE | re.DOTALL)\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        return \" \".join(match.group(1).strip().split())\n",
    "    return None\n",
    "\n",
    "# --- 1. Create the structured features ---\n",
    "df['title'] = df['catalog_content'].apply(lambda x: robust_extract_field(x, 'item name'))\n",
    "df['manufacturer'] = df['catalog_content'].apply(lambda x: robust_extract_field(x, 'manufacturer'))\n",
    "df['size'] = df['catalog_content'].apply(lambda x: robust_extract_field(x, 'size'))\n",
    "\n",
    "# --- 2. Clean and process the new features ---\n",
    "# Fill NaNs\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['manufacturer'] = df['manufacturer'].fillna('unknown')\n",
    "\n",
    "# A simple function to parse numbers from the 'size' column\n",
    "def parse_size(size_str):\n",
    "    if not isinstance(size_str, str):\n",
    "        return np.nan\n",
    "    # Find the first number (integer or float) in the string\n",
    "    match = re.search(r'(\\d+\\.?\\d*)', size_str)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return np.nan\n",
    "\n",
    "df['size_numeric'] = df['size'].apply(parse_size).fillna(0) # Fill missing sizes with 0\n",
    "\n",
    "# --- 3. Prepare data for the pipeline ---\n",
    "train_df = df.dropna(subset=['price'])\n",
    "X = train_df[['title', 'manufacturer', 'size_numeric']]\n",
    "y = train_df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 4. Build the Hybrid Pipeline ---\n",
    "# This preprocessor handles each feature type correctly and separately\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('title_tfidf', TfidfVectorizer(stop_words='english', max_features=5000), 'title'),\n",
    "        ('manufacturer_ohe', OneHotEncoder(handle_unknown='ignore'), ['manufacturer']),\n",
    "        ('size_scaler', StandardScaler(), ['size_numeric'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# --- 5. Train and Evaluate ---\n",
    "print(\"Training the Hybrid Feature Pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nEvaluating...\")\n",
    "y_pred = pipeline.predict(X_val)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "validation_smape = smape(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Performance Evaluation ---\")\n",
    "print(f\"Original Baseline SMAPE (raw text): 56.35\")\n",
    "print(f\"Hybrid Model SMAPE (structured features): {validation_smape:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186f5837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LightGBM pipeline on the log-transformed price...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.268675 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 148796\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 3203\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "Training complete.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "--- Final Performance Evaluation ---\n",
      "Original Baseline SMAPE (raw text, linear model): 56.35\n",
      "Final Model SMAPE (structured features, non-linear model): 57.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# --- Assume 'df' is the fully processed DataFrame from the previous step ---\n",
    "# It should have 'price', 'title', 'manufacturer', and 'size_numeric'\n",
    "\n",
    "# --- 1. Prepare data, ensuring no NaNs in target ---\n",
    "train_df = df.dropna(subset=['price'])\n",
    "X = train_df[['title', 'manufacturer', 'size_numeric']]\n",
    "y = train_df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- 2. Build the Hybrid Preprocessor (same as before) ---\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('title_tfidf', TfidfVectorizer(max_features=10000), 'title'),\n",
    "        ('manufacturer_ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['manufacturer']),\n",
    "        # Note: Tree models don't require feature scaling, so we can drop StandardScaler\n",
    "    ],\n",
    "    remainder='passthrough' # Keep size_numeric\n",
    ")\n",
    "\n",
    "# --- 3. Create the LightGBM Pipeline ---\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lgb.LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# --- 4. Train the model on the LOG-TRANSFORMED target ---\n",
    "print(\"Training the LightGBM pipeline on the log-transformed price...\")\n",
    "pipeline.fit(X_train, np.log1p(y_train))\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 5. Evaluate the model ---\n",
    "print(\"\\nEvaluating...\")\n",
    "# Predict on the log scale\n",
    "log_preds = pipeline.predict(X_val)\n",
    "# Convert predictions BACK to the original price scale\n",
    "y_pred = np.expm1(log_preds)\n",
    "y_pred[y_pred < 0] = 0 # Ensure no negative prices\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "validation_smape = smape(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Performance Evaluation ---\")\n",
    "print(f\"Original Baseline SMAPE (raw text, linear model): 56.35\")\n",
    "print(f\"Final Model SMAPE (structured features, non-linear model): {validation_smape:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5127b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the strong baseline (LGBM on raw text)...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.428752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1203571\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 28338\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "Training complete.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "--- Final Performance Evaluation ---\n",
      "Original Baseline SMAPE (Ridge on raw text): 56.35\n",
      "Strong Baseline SMAPE (LGBM on raw text): 55.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/.pyenv/versions/3.11.0/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Load original data ---\n",
    "df = pd.read_csv('/Users/adityasharma/Github Projects/Amazon/input/train.csv')\n",
    "df = df.dropna(subset=['price'])\n",
    "df['catalog_content'] = df['catalog_content'].astype(str).fillna('')\n",
    "\n",
    "# --- SMAPE Definition ---\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "# --- Data Preparation ---\n",
    "X = df['catalog_content']\n",
    "y = df['price']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- The Strong Baseline Pipeline ---\n",
    "# Combine the best features (raw text) with the best model (LightGBM)\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english', max_features=30000, ngram_range=(1, 2)),\n",
    "    lgb.LGBMRegressor(random_state=42, n_estimators=200, learning_rate=0.1)\n",
    ")\n",
    "\n",
    "# --- Train on the LOG-TRANSFORMED target ---\n",
    "print(\"Training the strong baseline (LGBM on raw text)...\")\n",
    "pipeline.fit(X_train, np.log1p(y_train))\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nEvaluating...\")\n",
    "log_preds = pipeline.predict(X_val)\n",
    "y_pred = np.expm1(log_preds)\n",
    "y_pred[y_pred < 0] = 0\n",
    "\n",
    "validation_smape = smape(y_val, y_pred)\n",
    "\n",
    "print(\"\\n--- Final Performance Evaluation ---\")\n",
    "print(f\"Original Baseline SMAPE (Ridge on raw text): 56.35\")\n",
    "print(f\"Strong Baseline SMAPE (LGBM on raw text): {validation_smape:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9a9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (housing-price-prediction)",
   "language": "python",
   "name": "housing-price-prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
