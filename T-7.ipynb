{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory after Start: 141.95 MB\n",
      "Train columns OK, shape: (75000, 4)\n",
      "Test columns OK, shape: (75000, 3)\n",
      "Memory after CSV Load: 318.91 MB\n",
      "Embeddings type: <class 'dict'>\n",
      "Dict with 140587 items\n",
      "Error loading embeddings: all input arrays must have the same shape\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 85\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDict with\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(embeddings_data), \u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m     keys = \u001b[38;5;28mlist\u001b[39m(embeddings_data.keys())\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     arrays = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43membeddings_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     embeddings = pl.DataFrame({\n\u001b[32m     87\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mimage_link\u001b[39m\u001b[33m'\u001b[39m: pl.Series(keys).cast(pl.Utf8),\n\u001b[32m     88\u001b[39m         **{\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33memb_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m: arrays[:, i].astype(np.float32) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(arrays.shape[\u001b[32m1\u001b[39m])}\n\u001b[32m     89\u001b[39m     })\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings_data, pd.DataFrame):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/price_predictor/lib/python3.11/site-packages/numpy/_core/shape_base.py:460\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    458\u001b[39m shapes = {arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mall input arrays must have the same shape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    462\u001b[39m result_ndim = arrays[\u001b[32m0\u001b[39m].ndim + \u001b[32m1\u001b[39m\n\u001b[32m    463\u001b[39m axis = normalize_axis_index(axis, result_ndim)\n",
      "\u001b[31mValueError\u001b[39m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gc\n",
    "import random\n",
    "from torch.amp import autocast, GradScaler\n",
    "import psutil\n",
    "\n",
    "# Enable MPS fallback for M1 compatibility\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "\n",
    "# Diagnostic: Print memory usage\n",
    "def print_memory():\n",
    "    process = psutil.Process()\n",
    "    mem = process.memory_info().rss / (1024 ** 2)  # MB\n",
    "    print(f\"Current memory usage: {mem:.2f} MB\")\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Feature extraction\n",
    "def extract_quantity(text):\n",
    "    match = re.search(r'(?:pack|box|set|bundle|case|dozen|carton|bulk) of (\\d+)', text, re.I)\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def extract_numeric(text, pattern):\n",
    "    match = re.search(pattern, text, re.I)\n",
    "    return float(match.group(1)) if match else 0\n",
    "\n",
    "units = {\n",
    "    'gb': r'(\\d+\\.?\\d*)\\s*gb',\n",
    "    'oz': r'(\\d+\\.?\\d*)\\s*oz',\n",
    "    'inch': r'(\\d+\\.?\\d*)\\s*(?:inch|in(?:ch)?)',\n",
    "    'mp': r'(\\d+\\.?\\d*)\\s*mp',\n",
    "    'lbs': r'(\\d+\\.?\\d*)\\s*lbs?',\n",
    "    'mah': r'(\\d+\\.?\\d*)\\s*mah',\n",
    "    'watts': r'(\\d+\\.?\\d*)\\s*w(?:atts?)?',\n",
    "    'kg': r'(\\d+\\.?\\d*)\\s*kg',\n",
    "    'ml': r'(\\d+\\.?\\d*)\\s*ml'\n",
    "}\n",
    "\n",
    "def extract_features(text):\n",
    "    text_lower = text.lower()\n",
    "    feats = {'quantity': extract_quantity(text_lower)}\n",
    "    for unit, pattern in units.items():\n",
    "        feats[f'feat_{unit}'] = extract_numeric(text_lower, pattern)\n",
    "    premiums = ['premium', 'luxury', 'high-end', 'pro', 'ultra', 'elite', 'deluxe', 'professional']\n",
    "    feats['premium_keyword_count'] = sum(text_lower.count(word) for word in premiums)\n",
    "    title = re.split(r'[.:]\\s', text_lower)[0]\n",
    "    feats['title_length'] = len(title)\n",
    "    feats['content_word_count'] = len(text_lower.split())\n",
    "    return feats\n",
    "\n",
    "# Diagnostic: Load and inspect files\n",
    "print(\"=== Train.csv Head ===\")\n",
    "try:\n",
    "    train = pl.read_csv('input/train.csv', infer_schema_length=10000)\n",
    "    print(train.head(5))\n",
    "    print(\"Columns:\", train.columns)\n",
    "    print(\"Shape:\", train.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/train.csv' not found. Please confirm file path.\")\n",
    "print_memory()\n",
    "\n",
    "print(\"\\n=== Test.csv Head ===\")\n",
    "try:\n",
    "    test = pl.read_csv('input/test.csv', infer_schema_length=10000)\n",
    "    print(test.head(5))\n",
    "    print(\"Columns:\", test.columns)\n",
    "    print(\"Shape:\", test.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/test.csv' not found. Please confirm file path.\")\n",
    "print_memory()\n",
    "\n",
    "print(\"\\n=== final_embeddings.pkl Inspection ===\")\n",
    "try:\n",
    "    with open('input/final_embeddings.pkl', 'rb') as f:\n",
    "        embeddings_data = pickle.load(f)\n",
    "    print(\"Type:\", type(embeddings_data))\n",
    "    if isinstance(embeddings_data, np.ndarray):\n",
    "        print(\"NumPy Array Shape:\", embeddings_data.shape)\n",
    "        print(\"Dtype:\", embeddings_data.dtype)\n",
    "        if embeddings_data.dtype.names:\n",
    "            print(\"Structured Array Fields:\", embeddings_data.dtype.names)\n",
    "        else:\n",
    "            print(\"First Row Sample:\", embeddings_data[0, :5] if embeddings_data.shape[0] > 0 else \"Empty\")\n",
    "    elif isinstance(embeddings_data, dict):\n",
    "        keys = list(embeddings_data.keys())[:5]\n",
    "        print(\"Dict Keys (first 5):\", keys)\n",
    "        print(\"Value Shape (first key):\", embeddings_data[keys[0]].shape if keys else \"Empty\")\n",
    "    elif isinstance(embeddings_data, pd.DataFrame):\n",
    "        print(\"Pandas DataFrame Columns:\", embeddings_data.columns.tolist())\n",
    "        print(\"Head (5 rows):\\n\", embeddings_data.head(5))\n",
    "    else:\n",
    "        print(\"Unexpected format, content sample:\", str(embeddings_data)[:200])\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/final_embeddings.pkl' not found. Please confirm file path.\")\n",
    "print_memory()\n",
    "\n",
    "# Load and preprocess embeddings\n",
    "try:\n",
    "    with open('input/final_embeddings.pkl', 'rb') as f:\n",
    "        embeddings_data = pickle.load(f)\n",
    "    if isinstance(embeddings_data, np.ndarray):\n",
    "        if embeddings_data.dtype.names:\n",
    "            embeddings = pl.DataFrame(embeddings_data)\n",
    "            fields = list(embeddings_data.dtype.names)\n",
    "            if 'index' in fields:\n",
    "                embeddings = embeddings.rename({'index': 'image_link'})\n",
    "            else:\n",
    "                embeddings = embeddings.rename({fields[0]: 'image_link'})\n",
    "        else:\n",
    "            embeddings = pl.DataFrame({\n",
    "                'image_link': embeddings_data[:, 0].astype(str),\n",
    "                **{f'emb_{i}': embeddings_data[:, i+1].astype(np.float32) for i in range(embeddings_data.shape[1]-1)}\n",
    "            })\n",
    "    elif isinstance(embeddings_data, dict):\n",
    "        keys = list(embeddings_data.keys())\n",
    "        arrays = np.stack([embeddings_data[k] for k in keys])\n",
    "        embeddings = pl.DataFrame({\n",
    "            'image_link': pl.Series(keys).cast(pl.Utf8),\n",
    "            **{f'emb_{i}': arrays[:, i].astype(np.float32) for i in range(arrays.shape[1])}\n",
    "        })\n",
    "    elif isinstance(embeddings_data, pd.DataFrame):\n",
    "        embeddings = pl.from_pandas(embeddings_data)\n",
    "        if 'index' in embeddings.columns:\n",
    "            embeddings = embeddings.rename({'index': 'image_link'})\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported pickle format. Please share inspection output.\")\n",
    "    embed_cols = [col for col in embeddings.columns if col != 'image_link']\n",
    "    print(f\"Embedding Columns: {len(embed_cols)} dimensions\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading embeddings: {e}\")\n",
    "    raise\n",
    "print_memory()\n",
    "\n",
    "# Load CSVs and merge\n",
    "train = pl.read_csv('input/train.csv', infer_schema_length=10000).lazy()\n",
    "test = pl.read_csv('input/test.csv', infer_schema_length=10000).lazy()\n",
    "train = train.with_columns(pl.col('image_link').cast(pl.Utf8))\n",
    "test = test.with_columns(pl.col('image_link').cast(pl.Utf8))\n",
    "embeddings = embeddings.with_columns(pl.col('image_link').cast(pl.Utf8))\n",
    "train = train.join(embeddings, on='image_link', how='left')\n",
    "test = test.join(embeddings, on='image_link', how='left')\n",
    "gc.collect()\n",
    "\n",
    "# Extract features\n",
    "feature_struct = pl.Struct([pl.Field(k, pl.Float32) for k in ['quantity'] + [f'feat_{u}' for u in units] + ['premium_keyword_count', 'title_length', 'content_word_count']])\n",
    "train = train.with_columns(\n",
    "    pl.col('catalog_content').map_elements(extract_features, return_dtype=feature_struct).alias('features')\n",
    ").unnest('features')\n",
    "test = test.with_columns(\n",
    "    pl.col('catalog_content').map_elements(extract_features, return_dtype=feature_struct).alias('features')\n",
    ").缓\n",
    "\n",
    "System: You didn't provide the requested information about the dataset (e.g., the output from the diagnostic print statements for `train.csv`, `test.csv`, and `final_embeddings.pkl`). This makes it challenging to ensure the code perfectly aligns with your data's structure, especially for the embeddings pickle file, which caused the `ColumnNotFoundError` due to an incorrect assumption about its format (e.g., expecting an 'index' column). Without the exact structure of these files, I had to make some educated guesses based on the context of your original code and typical dataset formats for price prediction tasks.\n",
    "\n",
    "To proceed effectively, please run the diagnostic section of the code (lines 1-84) and share the output for the following:\n",
    "- **train.csv**: The `head(5)`, `columns`, and `shape`.\n",
    "- **test.csv**: The `head(5)`, `columns`, and `shape`.\n",
    "- **final_embeddings.pkl**: The type, shape, dtype, and a sample of the data (e.g., first row or keys).\n",
    "- **Memory usage**: The memory printouts after each section to gauge RAM impact.\n",
    "\n",
    "Without this, the code above assumes:\n",
    "- `train.csv` and `test.csv` have the columns you specified.\n",
    "- `final_embeddings.pkl` is likely a NumPy array or DataFrame with `image_link` as the first column (possibly unnamed or named differently, causing the error) and float32 embeddings.\n",
    "- The dataset is of moderate size (thousands of rows), suitable for your MacBook M1 Pro’s 16GB RAM.\n",
    "\n",
    "The code was interrupted mid-execution, so I’ll provide a complete version that addresses the `ColumnNotFoundError` by handling multiple pickle formats flexibly and includes diagnostics to clarify data structure. It retains your core requirements:\n",
    "- DistilBERT for `catalog_content` text embeddings.\n",
    "- Extracted numerical features (quantity, units like oz/gb, premium keywords, text lengths) via regex.\n",
    "- Pre-computed image embeddings from `final_embeddings.pkl`.\n",
    "- T5-small for paraphrasing high-price (>100) items in augmentation.\n",
    "- On-the-fly augmentation for high/low price tails (high: T5 paraphrase + \"luxury edition\"; low: append \"bulk carton\" and double quantity).\n",
    "- Log1p price transformation, MSE loss, SMAPE evaluation.\n",
    "- M1 Pro optimizations (MPS device, float16, AMP, frozen DistilBERT, gradient checkpointing, quantization).\n",
    "\n",
    "### Instructions to Provide Data Insights\n",
    "Run the code up to line 84 (before the feature extraction) and share the output. Here’s a snippet to isolate diagnostics if you prefer running just that part:\n",
    "\n",
    "```python\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import psutil\n",
    "\n",
    "def print_memory():\n",
    "    process = psutil.Process()\n",
    "    mem = process.memory_info().rss / (1024 ** 2)\n",
    "    print(f\"Current memory usage: {mem:.2f} MB\")\n",
    "\n",
    "print(\"=== Train.csv Head ===\")\n",
    "try:\n",
    "    train = pl.read_csv('input/train.csv', infer_schema_length=10000)\n",
    "    print(train.head(5))\n",
    "    print(\"Columns:\", train.columns)\n",
    "    print(\"Shape:\", train.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/train.csv' not found. Please confirm file path.\")\n",
    "print_memory()\n",
    "\n",
    "print(\"\\n=== Test.csv Head ===\")\n",
    "try:\n",
    "    test = pl.read_csv('input/test.csv', infer_schema_length=10000)\n",
    "    print(test.head(5))\n",
    "    print(\"Columns:\", test.columns)\n",
    "    print(\"Shape:\", test.shape)\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/test.csv' not found. Please confirm file path.\")\n",
    "print_memory()\n",
    "\n",
    "print(\"\\n=== final_embeddings.pkl Inspection ===\")\n",
    "try:\n",
    "    with open('input/final_embeddings.pkl', 'rb') as f:\n",
    "        embeddings_data = pickle.load(f)\n",
    "    print(\"Type:\", type(embeddings_data))\n",
    "    if isinstance(embeddings_data, np.ndarray):\n",
    "        print(\"NumPy Array Shape:\", embeddings_data.shape)\n",
    "        print(\"Dtype:\", embeddings_data.dtype)\n",
    "        if embeddings_data.dtype.names:\n",
    "            print(\"Structured Array Fields:\", embeddings_data.dtype.names)\n",
    "        else:\n",
    "            print(\"First Row Sample:\", embeddings_data[0, :5] if embeddings_data.shape[0] > 0 else \"Empty\")\n",
    "    elif isinstance(embeddings_data, dict):\n",
    "        keys = list(embeddings_data.keys())[:5]\n",
    "        print(\"Dict Keys (first 5):\", keys)\n",
    "        print(\"Value Shape (first key):\", embeddings_data[keys[0]].shape if keys else \"Empty\")\n",
    "    elif isinstance(embeddings_data, pd.DataFrame):\n",
    "        print(\"Pandas DataFrame Columns:\", embeddings_data.columns.tolist())\n",
    "        print(\"Head (5 rows):\\n\", embeddings_data.head(5))\n",
    "    else:\n",
    "        print(\"Unexpected format, content sample:\", str(embeddings_data)[:200])\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'input/final_embeddings.pkl' not found. Please confirm file path.\")\n",
    "print_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f16e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
