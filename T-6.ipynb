{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3f236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import gc\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed precision\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Feature extraction\n",
    "def extract_quantity(text):\n",
    "    match = re.search(r'(?:pack|box|set|bundle|case|dozen|carton|bulk) of (\\d+)', text, re.I)\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def extract_numeric(text, pattern):\n",
    "    match = re.search(pattern, text, re.I)\n",
    "    return float(match.group(1)) if match else 0\n",
    "\n",
    "units = {\n",
    "    'gb': r'(\\d+\\.?\\d*)\\s*gb',\n",
    "    'oz': r'(\\d+\\.?\\d*)\\s*oz',\n",
    "    'inch': r'(\\d+\\.?\\d*)\\s*(?:inch|in(?:ch)?)',\n",
    "    'mp': r'(\\d+\\.?\\d*)\\s*mp',\n",
    "    'lbs': r'(\\d+\\.?\\d*)\\s*lbs?',\n",
    "    'mah': r'(\\d+\\.?\\d*)\\s*mah',\n",
    "    'watts': r'(\\d+\\.?\\d*)\\s*w(?:atts?)?',\n",
    "    'kg': r'(\\d+\\.?\\d*)\\s*kg',\n",
    "    'ml': r'(\\d+\\.?\\d*)\\s*ml'\n",
    "}\n",
    "\n",
    "def extract_features(row):\n",
    "    text = row['catalog_content'].lower()\n",
    "    feats = {'quantity': extract_quantity(text)}\n",
    "    for unit, pattern in units.items():\n",
    "        feats[f'feat_{unit}'] = extract_numeric(text, pattern)\n",
    "    premiums = ['premium', 'luxury', 'high-end', 'pro', 'ultra', 'elite', 'deluxe', 'professional']\n",
    "    feats['premium_keyword_count'] = sum(text.count(word) for word in premiums)\n",
    "    title = re.split(r'[.:]\\s', text)[0]\n",
    "    feats['title_length'] = len(title)\n",
    "    feats['content_word_count'] = len(text.split())\n",
    "    return pd.Series(feats)\n",
    "\n",
    "# Batched T5 paraphrasing\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "def paraphrase_batch(batch_texts):\n",
    "    input_texts = [f\"paraphrase: {text}\" for text in batch_texts]\n",
    "    inputs = t5_tokenizer(input_texts, return_tensors='pt', max_length=64, truncation=True, padding=True)\n",
    "    outputs = t5_model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=64, num_beams=4, early_stopping=True)\n",
    "    return [t5_tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Augmentation (train only, reduced frac and replication for RAM)\n",
    "def augment_data(df):\n",
    "    high = df[df['price'] > 100].copy()\n",
    "    low = df[df['price'] < 10].copy()\n",
    "    # Batched paraphrasing for highs\n",
    "    high_texts = high['catalog_content'].tolist()\n",
    "    batch_size = 4\n",
    "    paraphrased = []\n",
    "    for i in range(0, len(high_texts), batch_size):\n",
    "        batch = high_texts[i:i + batch_size]\n",
    "        paraphrased.extend(paraphrase_batch(batch))\n",
    "        gc.collect()\n",
    "    high['catalog_content'] = [p + ' luxury edition' for p in paraphrased]\n",
    "    low['catalog_content'] = low['catalog_content'] + ' bulk carton'\n",
    "    low['quantity'] = low['quantity'] * 2\n",
    "    high = pd.concat([high] * 3, ignore_index=True)\n",
    "    low = pd.concat([low] * 3, ignore_index=True)\n",
    "    augmented = pd.concat([df, high, low], ignore_index=True)\n",
    "    gc.collect()\n",
    "    return augmented\n",
    "\n",
    "# Load and preprocess\n",
    "train = pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')\n",
    "with open('input/final_embeddings.pkl', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "embeddings = pd.DataFrame(embeddings).reset_index().rename(columns={'index': 'image_link'})\n",
    "embed_cols = [f'emb_{i}' for i in range(embeddings.shape[1] - 1)]\n",
    "embeddings.columns = ['image_link'] + embed_cols\n",
    "train['image_link'] = train['image_link'].astype(str)\n",
    "test['image_link'] = test['image_link'].astype(str)\n",
    "embeddings['image_link'] = embeddings['image_link'].astype(str)\n",
    "train = train.merge(embeddings, on='image_link', how='left')\n",
    "test = test.merge(embeddings, on='image_link', how='left')\n",
    "gc.collect()\n",
    "train = pd.concat([train, train.apply(extract_features, axis=1)], axis=1)\n",
    "test = pd.concat([test, test.apply(extract_features, axis=1)], axis=1)\n",
    "gc.collect()\n",
    "\n",
    "# Split (val has no synthetic data)\n",
    "X = train.drop('price', axis=1)\n",
    "y = train['price']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)\n",
    "train = X_train.assign(price=y_train)\n",
    "train = augment_data(train)\n",
    "valid = X_valid.assign(price=y_valid)\n",
    "gc.collect()\n",
    "\n",
    "num_cols = ['quantity'] + [f'feat_{u}' for u in units] + ['premium_keyword_count', 'title_length', 'content_word_count'] + embed_cols\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler = StandardScaler()\n",
    "train[num_cols] = train[num_cols].astype(np.float32)\n",
    "valid[num_cols] = valid[num_cols].astype(np.float32)\n",
    "test[num_cols] = test[num_cols].astype(np.float32)\n",
    "train[num_cols] = imputer.fit_transform(train[num_cols])\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "valid[num_cols] = imputer.transform(valid[num_cols])\n",
    "valid[num_cols] = scaler.transform(valid[num_cols])\n",
    "test[num_cols] = imputer.transform(test[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "gc.collect()\n",
    "\n",
    "# Dataset\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=64):\n",
    "        self.texts = df['catalog_content'].values\n",
    "        self.numerics = df[num_cols].values.astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.prices = df['price'].values if 'price' in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'numerics': torch.tensor(self.numerics[idx], dtype=torch.float32)\n",
    "        }\n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(np.log1p(self.prices[idx]), dtype=torch.float32)\n",
    "        return item\n",
    "\n",
    "# Model\n",
    "class BertPriceRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.fc1 = nn.Linear(768 + len(num_cols), 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, numerics):\n",
    "        text_emb = self.text_model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat([text_emb, numerics], dim=1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Training\n",
    "device = torch.device('cpu')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_ds = PriceDataset(train, tokenizer)\n",
    "valid_ds = PriceDataset(valid, tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=2)\n",
    "model = BertPriceRegressor().to(device)\n",
    "model.half()  # Float16 mixed precision\n",
    "scaler_amp = GradScaler()\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = LinearLR(optimizer, start_factor=0.1, total_iters=100)\n",
    "loss_fn = nn.MSELoss()\n",
    "best_val_smape = float('inf')\n",
    "accumulation_steps = 4\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        with autocast():\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "            target = batch['price'].to(device)\n",
    "            pred = model(**inputs).squeeze()\n",
    "            loss = loss_fn(pred, target)\n",
    "            loss = loss / accumulation_steps\n",
    "        scaler_amp.scale(loss).backward()\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            scaler_amp.step(optimizer)\n",
    "            scaler_amp.update()\n",
    "            optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    preds, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            with autocast():\n",
    "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "                target = batch['price'].to(device)\n",
    "                pred = model(**inputs).squeeze()\n",
    "            preds.extend(np.expm1(pred.cpu().numpy()))\n",
    "            actuals.extend(np.expm1(target.cpu().numpy()))\n",
    "    val_smape = smape(np.array(actuals), np.array(preds))\n",
    "    if val_smape < best_val_smape:\n",
    "        best_val_smape = val_smape\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    gc.collect()\n",
    "print(f'T6 Validation SMAPE: {best_val_smape}')\n",
    "\n",
    "# Error Analysis\n",
    "errors = pd.DataFrame({\n",
    "    'actual': actuals,\n",
    "    'pred': preds,\n",
    "    'diff': np.array(preds) - np.array(actuals),\n",
    "    'ape': 2 * 100 * np.abs(np.array(preds) - np.array(actuals)) / (np.abs(actuals) + np.abs(preds))\n",
    "})\n",
    "bins = [0, 10, 50, 100, 500, np.inf]\n",
    "labels = ['0-10 (Low/Bulk)', '10-50', '50-100', '100-500 (High)', '500+ (Extreme)']\n",
    "errors['price_bin'] = pd.cut(errors['actual'], bins=bins, labels=labels)\n",
    "bin_smape = errors.groupby('price_bin')['ape'].mean() / 2\n",
    "print('SMAPE per bin:\\n', bin_smape)\n",
    "bin_bias = errors.groupby('price_bin')['diff'].mean()\n",
    "print('Bias per bin:\\n', bin_bias)\n",
    "bin_var = errors.groupby('price_bin')['diff'].var()\n",
    "print('Error variance per bin:\\n', bin_var)\n",
    "top_errors = errors.sort_values('ape', ascending=False).head(10)\n",
    "print('Top 10 worst predictions:\\n', top_errors)\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = f'output/T6_{timestamp}'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(out_dir, 'model.pt'))\n",
    "pd.DataFrame({'actual': actuals, 'pred': preds}).to_csv(os.path.join(out_dir, 'preds_valid.csv'), index=False)\n",
    "with open(os.path.join(out_dir, 'error_analysis.txt'), 'w') as f:\n",
    "    f.write(f'Validation SMAPE: {best_val_smape}\\n')\n",
    "    f.write('SMAPE per bin:\\n' + str(bin_smape) + '\\n')\n",
    "    f.write('Bias per bin:\\n' + str(bin_bias) + '\\n')\n",
    "    f.write('Error variance per bin:\\n' + str(bin_var) + '\\n')\n",
    "    f.write('Top 10 worst:\\n' + str(top_errors) + '\\n')\n",
    "\n",
    "# Test inference\n",
    "test_ds = PriceDataset(test, tokenizer)\n",
    "test_loader = DataLoader(test_ds, batch_size=2)\n",
    "model.eval()\n",
    "pred_test = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        with autocast():\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            pred = model(**inputs).squeeze()\n",
    "        pred_test.extend(np.expm1(pred.cpu().numpy()))\n",
    "submission = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test})\n",
    "submission.to_csv(os.path.join(out_dir, 'submission.csv'), index=False)\n",
    "print(f'Outputs saved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9753152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
