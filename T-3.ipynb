{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc625ee1",
   "metadata": {},
   "source": [
    "Features:\n",
    "\n",
    "Text: DistilBERT embeddings (768 dims) on catalog_content.\n",
    "Numerics: Your V16 set (quantity, feat_gb/oz/inch/mp/lbs/mah/watts, premium_keyword_count, condition_flag, title_length, content_word_count).\n",
    "Image: ViT-base (HuggingFace) embeddings (768 dims) from image_link URLs (assumes download capability; else placeholder).\n",
    "\n",
    "\n",
    "Model: Custom PyTorch model:\n",
    "\n",
    "DistilBERT for text (freeze lower layers).\n",
    "ViT for images (freeze).\n",
    "Cross-attention: Text embeddings attend to image embeddings to weigh relevance.\n",
    "Concat with scaled numerics.\n",
    "Linear layers â†’ Tweedie output (log-link for price).\n",
    "\n",
    "\n",
    "Loss: Tweedie (variance_power=1.5, tune 1.2-1.8) to handle skew natively.\n",
    "Training:\n",
    "\n",
    "Tail weights: 3x for >100, 2x for <10.\n",
    "AdamW, linear scheduler with warmup.\n",
    "Early stopping on validation SMAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef253c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496a1d6141bb43f2baccd45fc4754ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08df910fb9f94877b73093be6738cb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72077068de7a40b298253adf9b88eec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, ViTImageProcessor, ViTModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Feature extraction\n",
    "def extract_quantity(text):\n",
    "    match = re.search(r'(?:pack|box|set|bundle|case|dozen|carton) of (\\d+)', text, re.I)\n",
    "    return int(match.group(1)) if match else 1\n",
    "\n",
    "def extract_numeric(text, pattern):\n",
    "    match = re.search(pattern, text, re.I)\n",
    "    return float(match.group(1)) if match else 0\n",
    "\n",
    "units = {\n",
    "    'gb': r'(\\d+\\.?\\d*)\\s*gb',\n",
    "    'oz': r'(\\d+\\.?\\d*)\\s*oz',\n",
    "    'inch': r'(\\d+\\.?\\d*)\\s*(?:inch|in(?:ch)?)',\n",
    "    'mp': r'(\\d+\\.?\\d*)\\s*mp',\n",
    "    'lbs': r'(\\d+\\.?\\d*)\\s*lbs?',\n",
    "    'mah': r'(\\d+\\.?\\d*)\\s*mah',\n",
    "    'watts': r'(\\d+\\.?\\d*)\\s*w(?:atts?)?',\n",
    "    'kg': r'(\\d+\\.?\\d*)\\s*kg'\n",
    "}\n",
    "\n",
    "def extract_features(row):\n",
    "    text = row['catalog_content'].lower()\n",
    "    feats = {'quantity': extract_quantity(text)}\n",
    "    for unit, pattern in units.items():\n",
    "        feats[f'feat_{unit}'] = extract_numeric(text, pattern)\n",
    "    premiums = ['premium', 'luxury', 'high-end', 'pro', 'ultra', 'elite', 'deluxe', 'professional']\n",
    "    feats['premium_keyword_count'] = sum(text.count(word) for word in premiums)\n",
    "    if re.search(r'\\bnew\\b|\\bmint\\b|\\bbrand new\\b', text):\n",
    "        feats['condition_flag'] = 1\n",
    "    elif re.search(r'\\bused\\b|\\brefurbished\\b|\\bpre-owned\\b', text):\n",
    "        feats['condition_flag'] = 0\n",
    "    else:\n",
    "        feats['condition_flag'] = 0.5\n",
    "    title = re.split(r'[.:]\\s', text)[0]\n",
    "    feats['title_length'] = len(title)\n",
    "    feats['content_word_count'] = len(text.split())\n",
    "    return pd.Series(feats)\n",
    "\n",
    "# Augmentation\n",
    "def augment_data(df):\n",
    "    high = df[df['price'] > 100].copy()\n",
    "    low = df[df['price'] < 10].copy()\n",
    "    high['catalog_content'] = high['catalog_content'] + ' luxury edition'\n",
    "    low['quantity'] = low['quantity'] * 2\n",
    "    return pd.concat([df, high, low], ignore_index=True)\n",
    "\n",
    "# Load and preprocess\n",
    "train = pd.read_csv('input/train.csv')\n",
    "train = pd.concat([train, train.apply(extract_features, axis=1)], axis=1)\n",
    "train = augment_data(train)\n",
    "\n",
    "num_cols = ['quantity'] + [f'feat_{u}' for u in units] + ['premium_keyword_count', 'condition_flag', 'title_length', 'content_word_count']\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "scaler = StandardScaler()\n",
    "train[num_cols] = imputer.fit_transform(train[num_cols])\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "\n",
    "# Split\n",
    "X = train.drop('price', axis=1)\n",
    "y = train['price']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_valid = y_valid.reset_index(drop=True)\n",
    "\n",
    "# Dataset\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, image_processor, max_length=128):\n",
    "        self.texts = df['catalog_content'].values\n",
    "        self.numerics = df[num_cols].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_length = max_length\n",
    "        self.image_links = df['image_link'].values\n",
    "        self.prices = df['price'].values if 'price' in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n",
    "        try:\n",
    "            response = requests.get(self.image_links[idx], timeout=5)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            image = self.image_processor(image, return_tensors='pt')\n",
    "        except:\n",
    "            image = {'pixel_values': torch.zeros(1, 3, 224, 224)}\n",
    "        numerics = torch.tensor(self.numerics[idx], dtype=torch.float)\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'pixel_values': image['pixel_values'].squeeze(),\n",
    "            'numerics': numerics\n",
    "        }\n",
    "        if self.prices is not None:\n",
    "            item['price'] = torch.tensor(self.prices[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Model\n",
    "class TweedieLoss(nn.Module):\n",
    "    def __init__(self, p=1.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        term1 = -target * torch.pow(pred, 1 - self.p) / (1 - self.p)\n",
    "        term2 = torch.pow(pred, 2 - self.p) / (2 - self.p)\n",
    "        return torch.mean(term1 + term2)\n",
    "\n",
    "class PriceRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=768, num_heads=8)\n",
    "        self.fc1 = nn.Linear(768 + len(num_cols), 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, pixel_values, numerics):\n",
    "        text_emb = self.text_model(input_ids, attention_mask=attention_mask).last_hidden_state[:, 0, :]\n",
    "        image_emb = self.image_model(pixel_values).last_hidden_state[:, 0, :]\n",
    "        text_emb = text_emb.unsqueeze(0)\n",
    "        image_emb = image_emb.unsqueeze(0)\n",
    "        attn_output, _ = self.cross_attn(text_emb, image_emb, image_emb)\n",
    "        fused = attn_output.squeeze(0) + text_emb.squeeze(0)\n",
    "        combined = torch.cat([fused, numerics], dim=1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "train_ds = PriceDataset(X_train.assign(price=y_train), tokenizer, image_processor)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "valid_ds = PriceDataset(X_valid.assign(price=y_valid), tokenizer, image_processor)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=8)\n",
    "\n",
    "model = PriceRegressor().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1, total_iters=100)\n",
    "loss_fn = TweedieLoss(p=1.5)\n",
    "weights = torch.ones(len(y_train), device=device)\n",
    "weights[y_train > 100] = 3\n",
    "weights[y_train < 10] = 2\n",
    "\n",
    "# Train\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "        target = batch['price'].to(device)\n",
    "        pred = model(**inputs).squeeze()\n",
    "        loss = loss_fn(pred, target)\n",
    "        batch_weights = weights[batch_idx * train_loader.batch_size:(batch_idx + 1) * train_loader.batch_size]\n",
    "        loss = (loss * batch_weights).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "# Eval\n",
    "model.eval()\n",
    "preds, actuals = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k != 'price'}\n",
    "        target = batch['price'].to(device)\n",
    "        pred = model(**inputs).squeeze()\n",
    "        preds.extend(np.expm1(pred.cpu().numpy()))\n",
    "        actuals.extend(np.expm1(target.cpu().numpy()))\n",
    "smape_score = smape(np.array(actuals), np.array(preds))\n",
    "print(f'T3 Validation SMAPE: {smape_score}')\n",
    "\n",
    "# Error Analysis\n",
    "errors = pd.DataFrame({\n",
    "    'actual': actuals,\n",
    "    'pred': preds,\n",
    "    'diff': np.array(preds) - np.array(actuals),\n",
    "    'ape': 2 * 100 * np.abs(np.array(preds) - np.array(actuals)) / (np.abs(actuals) + np.abs(preds))\n",
    "})\n",
    "bins = [0, 10, 50, 100, 500, np.inf]\n",
    "labels = ['0-10 (Low/Bulk)', '10-50', '50-100', '100-500 (High)', '500+ (Extreme)']\n",
    "errors['price_bin'] = pd.cut(errors['actual'], bins=bins, labels=labels)\n",
    "bin_smape = errors.groupby('price_bin')['ape'].mean() / 2\n",
    "print('SMAPE per bin:\\n', bin_smape)\n",
    "bin_bias = errors.groupby('price_bin')['diff'].mean()\n",
    "print('Bias (pred - actual) per bin:\\n', bin_bias)\n",
    "bin_var = errors.groupby('price_bin')['diff'].var()\n",
    "print('Error variance per bin:\\n', bin_var)\n",
    "top_errors = errors.sort_values('ape', ascending=False).head(10)\n",
    "print('Top 10 worst predictions:\\n', top_errors)\n",
    "if bin_bias.iloc[-2] < 0:\n",
    "    print('Insight: Underpredicting highsâ€”try higher ViT weight or more high-price augmentation.')\n",
    "if bin_smape.iloc[0] > bin_smape.mean():\n",
    "    print('Insight: Bulk errors highâ€”refine quantity regex or augment more low-price samples.')\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "out_dir = f'output/T3_{timestamp}'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.save(model.state_dict(), os.path.join(out_dir, 'model.pt'))\n",
    "pd.DataFrame({'actual': actuals, 'pred': preds}).to_csv(os.path.join(out_dir, 'preds_valid.csv'), index=False)\n",
    "with open(os.path.join(out_dir, 'error_analysis.txt'), 'w') as f:\n",
    "    f.write(f'Validation SMAPE: {smape_score}\\n')\n",
    "    f.write('SMAPE per bin:\\n' + str(bin_smape) + '\\n')\n",
    "    f.write('Bias per bin:\\n' + str(bin_bias) + '\\n')\n",
    "    f.write('Error variance per bin:\\n' + str(bin_var) + '\\n')\n",
    "    f.write('Top 10 worst:\\n' + str(top_errors) + '\\n')\n",
    "\n",
    "# Test inference\n",
    "test = pd.read_csv('input/test.csv')\n",
    "test = pd.concat([test, test.apply(extract_features, axis=1)], axis=1)\n",
    "test[num_cols] = imputer.transform(test[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "test_ds = PriceDataset(test, tokenizer, image_processor)\n",
    "test_loader = DataLoader(test_ds, batch_size=8)\n",
    "model.eval()\n",
    "pred_test = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        pred = model(**inputs).squeeze()\n",
    "        pred_test.extend(np.expm1(pred.cpu().numpy()))\n",
    "submission = pd.DataFrame({'sample_id': test['sample_id'], 'price': pred_test})\n",
    "submission.to_csv(os.path.join(out_dir, 'submission.csv'), index=False)\n",
    "print(f'Outputs saved to {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185b366b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
