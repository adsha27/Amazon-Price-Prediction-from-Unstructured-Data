{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9981b2e2",
   "metadata": {},
   "source": [
    "Instead of one model for everything, we'll build a system:\n",
    "\n",
    "Stage 1 (The Router): A simple, fast classifier will first look at a product and predict if it's \"low-price\" (e.g., < $100) or \"high-price\" (e.g., >= $100).\n",
    "\n",
    "Stage 2 (The Experts): We will train two separate, highly specialized LightGBM regressors:\n",
    "\n",
    "One expert trained only on low-priced items.\n",
    "\n",
    "Another expert trained only on high-priced items.\n",
    "\n",
    "During prediction, the router first decides which price category the product belongs to, and then passes it to the appropriate expert for a precise price prediction. This forces our system to learn the distinct patterns of high-value items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9043c353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Step 1/9] Pre-processing embeddings for fast lookup ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9090d6beca84d49999ef2a06e1f3fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping Embeddings:   0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix created for fast indexing.\n",
      "\n",
      "--- [Step 2/9] Loading Data and Engineering Features ---\n",
      "\n",
      "--- [Step 3/9] Splitting Data into Train/Validation Sets ---\n",
      "\n",
      "--- [Step 4/9] Constructing Feature Matrices ---\n",
      "\n",
      "--- [Step 5/9] Training Stage 1: Price Category Classifier ---\n",
      "\n",
      "--- [Step 6/9] Training Stage 2: Expert Regressors ---\n",
      "Training low-price expert...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.547821 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1761914\n",
      "[LightGBM] [Info] Number of data points in the train set: 58486, number of used features: 29989\n",
      "[LightGBM] [Info] Start training from score 2.681081\n",
      "Training high-price expert...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 185561\n",
      "[LightGBM] [Info] Number of data points in the train set: 1514, number of used features: 2736\n",
      "[LightGBM] [Info] Start training from score 5.007482\n",
      "\n",
      "--- [Step 7/9] Evaluating V14 Mixture of Experts Model ---\n",
      "V14 (Mixture of Experts) SMAPE: 56.9696\n",
      "\n",
      "--- [Step 8/9] Saving V14 Model Artifacts ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/miniforge3/envs/price_predictor/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All V14 model components saved to: output/embeddings_model_v14.pkl\n",
      "\n",
      "--- [Step 9/9] Generating Final Submission File ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/miniforge3/envs/price_predictor/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission.csv created successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL V14 SCRIPT: MIXTURE OF EXPERTS FOR PRICE PREDICTION\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Justification: Centralized parameters for easy modification and reproducibility.\n",
    "INPUT_DIR = 'input/'\n",
    "OUTPUT_DIR = 'output/'\n",
    "MODEL_VERSION = 'embeddings_model_v14'\n",
    "PRICE_THRESHOLD = 100.0  # Threshold to distinguish low vs. high price items\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 2. HELPER FUNCTION ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "\n",
    "# --- 3. OPTIMIZED EMBEDDING PRE-PROCESSING ---\n",
    "print(\"--- [Step 1/9] Pre-processing embeddings for fast lookup ---\")\n",
    "# Justification: This one-time process avoids slow, repeated lookups later.\n",
    "# We create a matrix where the row index directly corresponds to the sample_id.\n",
    "try:\n",
    "    train_df_full = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'), index_col='sample_id')\n",
    "    test_df_full = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'), index_col='sample_id')\n",
    "    with open(os.path.join(INPUT_DIR, 'final_embeddings.pkl'), 'rb') as f:\n",
    "        image_embeddings_dict = pickle.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}. Please check your INPUT_DIR path.\")\n",
    "    exit()\n",
    "\n",
    "all_df = pd.concat([train_df_full, test_df_full])\n",
    "max_id = all_df.index.max()\n",
    "embedding_dim = 512  # From our CLIP model\n",
    "embedding_matrix = np.zeros((max_id + 1, embedding_dim), dtype=np.float32)\n",
    "\n",
    "for sample_id, row in tqdm(all_df.iterrows(), total=len(all_df), desc=\"Mapping Embeddings\"):\n",
    "    embedding = image_embeddings_dict.get(row['image_link'])\n",
    "    if embedding is not None:\n",
    "        embedding_matrix[sample_id] = embedding\n",
    "print(\"Embedding matrix created for fast indexing.\")\n",
    "\n",
    "\n",
    "# --- 4. DATA LOADING & FEATURE ENGINEERING ---\n",
    "print(\"\\n--- [Step 2/9] Loading Data and Engineering Features ---\")\n",
    "train_df = train_df_full.dropna(subset=['catalog_content', 'price', 'image_link']).copy()\n",
    "train_df['is_high_price'] = (train_df['price'] >= PRICE_THRESHOLD).astype(int)\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "\n",
    "units = ['gb', 'oz', 'inch', 'mah', 'count', 'pack']\n",
    "for unit in units:\n",
    "    regex = r'(\\d+\\.?\\d*)\\s?' + re.escape(unit)\n",
    "    train_df[f'feat_{unit}'] = train_df['catalog_content'].str.extract(regex, flags=re.IGNORECASE).astype(float).fillna(0)\n",
    "\n",
    "\n",
    "# --- 5. TRAIN/VALIDATION SPLIT ---\n",
    "print(\"\\n--- [Step 3/9] Splitting Data into Train/Validation Sets ---\")\n",
    "X_train_df, X_val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=train_df['is_high_price']\n",
    ")\n",
    "\n",
    "\n",
    "# --- 6. FEATURE MATRIX CONSTRUCTION ---\n",
    "print(\"\\n--- [Step 4/9] Constructing Feature Matrices ---\")\n",
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = [f'feat_{unit}' for unit in units]\n",
    "\n",
    "# Fit on training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_df['catalog_content'])\n",
    "X_train_num = scaler.fit_transform(X_train_df[numerical_cols])\n",
    "X_train_img = embedding_matrix[X_train_df.index] # Fast lookup\n",
    "X_train_full = hstack([X_train_tfidf, X_train_num, X_train_img], format='csr')\n",
    "\n",
    "# Transform validation data\n",
    "X_val_tfidf = tfidf.transform(X_val_df['catalog_content'])\n",
    "X_val_num = scaler.transform(X_val_df[numerical_cols])\n",
    "X_val_img = embedding_matrix[X_val_df.index] # Fast lookup\n",
    "X_val_full = hstack([X_val_tfidf, X_val_num, X_val_img], format='csr')\n",
    "\n",
    "\n",
    "# --- 7. STAGE 1: TRAIN THE ROUTER (CLASSIFIER) ---\n",
    "print(\"\\n--- [Step 5/9] Training Stage 1: Price Category Classifier ---\")\n",
    "router_model = LogisticRegression(random_state=RANDOM_STATE, solver='liblinear', class_weight='balanced')\n",
    "router_model.fit(X_train_tfidf, X_train_df['is_high_price'])\n",
    "\n",
    "\n",
    "# --- 8. STAGE 2: TRAIN THE EXPERTS (REGRESSORS) ---\n",
    "print(\"\\n--- [Step 6/9] Training Stage 2: Expert Regressors ---\")\n",
    "low_price_indices_train = X_train_df[X_train_df['is_high_price'] == 0].index\n",
    "high_price_indices_train = X_train_df[X_train_df['is_high_price'] == 1].index\n",
    "\n",
    "# Train Low-Price Expert\n",
    "print(\"Training low-price expert...\")\n",
    "lgbm_low_price = lgb.LGBMRegressor(random_state=RANDOM_STATE)\n",
    "lgbm_low_price.fit(X_train_full[X_train_df.index.isin(low_price_indices_train)], X_train_df.loc[low_price_indices_train, 'log_price'])\n",
    "\n",
    "# Train High-Price Expert\n",
    "# Justification: Using more estimators for the smaller high-price dataset can help it learn more complex patterns.\n",
    "print(\"Training high-price expert...\")\n",
    "lgbm_high_price = lgb.LGBMRegressor(random_state=RANDOM_STATE, n_estimators=200, learning_rate=0.05)\n",
    "lgbm_high_price.fit(X_train_full[X_train_df.index.isin(high_price_indices_train)], X_train_df.loc[high_price_indices_train, 'log_price'])\n",
    "\n",
    "\n",
    "# --- 9. EVALUATION ON VALIDATION SET ---\n",
    "print(\"\\n--- [Step 7/9] Evaluating V14 Mixture of Experts Model ---\")\n",
    "val_categories = router_model.predict(X_val_tfidf)\n",
    "val_predictions = np.zeros(len(X_val_df))\n",
    "\n",
    "low_mask = (val_categories == 0)\n",
    "high_mask = (val_categories == 1)\n",
    "\n",
    "if low_mask.any():\n",
    "    val_predictions[low_mask] = lgbm_low_price.predict(X_val_full[low_mask])\n",
    "if high_mask.any():\n",
    "    val_predictions[high_mask] = lgbm_high_price.predict(X_val_full[high_mask])\n",
    "\n",
    "final_preds = np.expm1(val_predictions)\n",
    "final_preds[final_preds < 0] = 0\n",
    "v14_smape = smape(X_val_df['price'], final_preds)\n",
    "\n",
    "print(f\"V14 (Mixture of Experts) SMAPE: {v14_smape:.4f}\")\n",
    "\n",
    "\n",
    "# --- 10. SAVE MODEL ARTIFACTS ---\n",
    "print(\"\\n--- [Step 8/9] Saving V14 Model Artifacts ---\")\n",
    "artifacts = {\n",
    "    'router': router_model,\n",
    "    'expert_low_price': lgbm_low_price,\n",
    "    'expert_high_price': lgbm_high_price,\n",
    "    'tfidf_vectorizer': tfidf,\n",
    "    'numerical_scaler': scaler,\n",
    "    'numerical_columns': numerical_cols,\n",
    "    'price_threshold': PRICE_THRESHOLD,\n",
    "    'version': MODEL_VERSION\n",
    "}\n",
    "output_path = os.path.join(OUTPUT_DIR, f\"{MODEL_VERSION}.pkl\")\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "print(f\"All V14 model components saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# --- 11. GENERATE SUBMISSION FILE ---\n",
    "print(\"\\n--- [Step 9/9] Generating Final Submission File ---\")\n",
    "# This section shows how you would use the saved artifacts to predict on the test set.\n",
    "# Prepare test features\n",
    "test_df = test_df_full.dropna(subset=['catalog_content', 'image_link']).copy()\n",
    "for unit in units:\n",
    "    regex = r'(\\d+\\.?\\d*)\\s?' + re.escape(unit)\n",
    "    test_df[f'feat_{unit}'] = test_df['catalog_content'].str.extract(regex, flags=re.IGNORECASE).astype(float).fillna(0)\n",
    "\n",
    "X_test_tfidf = tfidf.transform(test_df['catalog_content'])\n",
    "X_test_num = scaler.transform(test_df[numerical_cols])\n",
    "X_test_img = embedding_matrix[test_df.index]\n",
    "X_test_full = hstack([X_test_tfidf, X_test_num, X_test_img], format='csr')\n",
    "\n",
    "# Predict categories and route to experts\n",
    "test_categories = router_model.predict(X_test_tfidf)\n",
    "test_predictions_log = np.zeros(len(test_df))\n",
    "low_mask_test = (test_categories == 0)\n",
    "high_mask_test = (test_categories == 1)\n",
    "\n",
    "if low_mask_test.any():\n",
    "    test_predictions_log[low_mask_test] = lgbm_low_price.predict(X_test_full[low_mask_test])\n",
    "if high_mask_test.any():\n",
    "    test_predictions_log[high_mask_test] = lgbm_high_price.predict(X_test_full[high_mask_test])\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(index=test_df.index)\n",
    "submission_df['price'] = np.expm1(test_predictions_log)\n",
    "submission_df['price'] = submission_df['price'].clip(0) # Ensure no negative prices\n",
    "submission_df.to_csv('submission.csv')\n",
    "\n",
    "print(\"\\nSubmission.csv created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d00c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- [Diagnostic] Testing a Single LGBM on V8 + Image Features ---\n",
      "Training a single LightGBM model on all features...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 4.460737 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1794068\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 30038\n",
      "[LightGBM] [Info] Start training from score 2.739783\n",
      "Evaluating the single model...\n",
      "\n",
      "--- Ablation Study Results ---\n",
      "V8 SMAPE (TF-IDF + Numericals only): 50.96\n",
      "V14 SMAPE (MoE on All Features): 56.97\n",
      "V15 SMAPE (Single LGBM on All Features): 56.9696\n",
      "\n",
      "Conclusion: The image embeddings themselves are adding noise and degrading performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/miniforge3/envs/price_predictor/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# --- V15 DIAGNOSTIC TEST: SINGLE MODEL WITH ALL FEATURES ---\n",
    "\n",
    "print(\"\\n--- [Diagnostic] Testing a Single LGBM on V8 + Image Features ---\")\n",
    "\n",
    "# We already have the feature matrices from the V14 script:\n",
    "# X_train_full (hstack of TF-IDF, numericals, and images)\n",
    "# X_val_full (hstack of TF-IDF, numericals, and images)\n",
    "# y_train_log (the log-transformed price from the train set)\n",
    "# X_val_df['price'] (the true price for the validation set)\n",
    "\n",
    "# Train a single, standard LightGBM Regressor\n",
    "single_lgbm_model = lgb.LGBMRegressor(random_state=RANDOM_STATE)\n",
    "print(\"Training a single LightGBM model on all features...\")\n",
    "single_lgbm_model.fit(X_train_full, X_train_df['log_price'])\n",
    "\n",
    "# Evaluate the single model\n",
    "print(\"Evaluating the single model...\")\n",
    "preds_log = single_lgbm_model.predict(X_val_full)\n",
    "preds = np.expm1(preds_log)\n",
    "preds[preds < 0] = 0\n",
    "\n",
    "v15_smape = smape(X_val_df['price'], final_preds)\n",
    "\n",
    "print(f\"\\n--- Ablation Study Results ---\")\n",
    "print(f\"V8 SMAPE (TF-IDF + Numericals only): 50.96\")\n",
    "print(f\"V14 SMAPE (MoE on All Features): 56.97\")\n",
    "print(f\"V15 SMAPE (Single LGBM on All Features): {v15_smape:.4f}\")\n",
    "\n",
    "if v15_smape < 52:\n",
    "     print(\"\\nConclusion: The Mixture of Experts architecture was the primary cause of failure.\")\n",
    "else:\n",
    "     print(\"\\nConclusion: The image embeddings themselves are adding noise and degrading performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb36d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- Running Error Diagnostic ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"--- Running Error Diagnostic ---\")\n",
    "\n",
    "# 1. Make predictions on the validation set (if not already done)\n",
    "# Ensure you have 'preds' from the V15 experiment\n",
    "try:\n",
    "    preds\n",
    "except NameError:\n",
    "    print(\"Predictions not found. Rerunning prediction step...\")\n",
    "    preds_log = single_lgbm_model.predict(X_val_full)\n",
    "    preds = np.expm1(preds_log)\n",
    "    preds[preds < 0] = 0\n",
    "\n",
    "# 2. Create an Error Analysis DataFrame\n",
    "error_df = X_val_df[['catalog_content', 'price']].copy()\n",
    "error_df['predicted_price'] = preds\n",
    "\n",
    "# Calculate the individual SMAPE component for each prediction\n",
    "numerator = np.abs(error_df['predicted_price'] - error_df['price'])\n",
    "denominator = (np.abs(error_df['price']) + np.abs(error_df['predicted_price'])) / 2\n",
    "error_df['smape_error'] = (numerator / (denominator + 1e-8)) * 100 # Add epsilon for stability\n",
    "\n",
    "# 3. Group by Price Brackets\n",
    "# Define the bins for our price ranges\n",
    "price_bins = [0, 10, 25, 50, 100, error_df['price'].max() + 1]\n",
    "bin_labels = ['0-10', '10-25', '25-50', '50-100', '100+']\n",
    "error_df['price_bracket'] = pd.cut(error_df['price'], bins=price_bins, labels=bin_labels, right=False)\n",
    "\n",
    "# 4. Analyze the Error per Bracket\n",
    "# Calculate the mean SMAPE for each group\n",
    "error_summary = error_df.groupby('price_bracket')['smape_error'].mean().reset_index()\n",
    "error_summary.rename(columns={'smape_error': 'average_smape'}, inplace=True)\n",
    "\n",
    "# Also count the number of samples in each bracket\n",
    "error_summary['sample_count'] = error_df.groupby('price_bracket').size().values\n",
    "\n",
    "print(\"\\n--- Error Analysis Summary ---\")\n",
    "print(error_summary)\n",
    "\n",
    "# 5. Visualize the Results\n",
    "print(\"\\n--- Visualizing Error Distribution ---\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='price_bracket', y='average_smape', data=error_summary, palette='viridis')\n",
    "plt.title('Average SMAPE per Price Bracket', fontsize=16)\n",
    "plt.xlabel('True Price Bracket ($)', fontsize=12)\n",
    "plt.ylabel('Average SMAPE', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Display some examples of the worst predictions in the high-price bracket\n",
    "print(\"\\n--- Examples of High-Error Predictions in the '100+' Bracket ---\")\n",
    "high_price_errors = error_df[error_df['price_bracket'] == '100+'].sort_values(by='smape_error', ascending=False)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "display(high_price_errors[['price', 'predicted_price', 'smape_error', 'catalog_content']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba0262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
