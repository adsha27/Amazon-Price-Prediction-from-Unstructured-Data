{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0c96d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data and Preparing Features ---\n",
      "\n",
      "--- Training V8 Model (Text + Numericals only) ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.186656 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1664300\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 29535\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "\n",
      "--- Training V15 Model (Text + Numericals + Images) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e634898d664660b72b0e72254f8927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping Embeddings: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.437131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1794860\n",
      "[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 30047\n",
      "[LightGBM] [Info] Start training from score 2.740904\n",
      "\n",
      "--- Generating Predictions for Both Models on Validation Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityasharma/miniforge3/envs/price_predictor/lib/python3.11/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Diagnostic: Comparing Performance on V8's 50 Worst Predictions ---\n",
      "Analysis of the 50 samples where the V8 (text-only) model performed worst:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>v8_pred</th>\n",
       "      <th>v8_error</th>\n",
       "      <th>v15_pred</th>\n",
       "      <th>v15_error</th>\n",
       "      <th>error_diff (v15 - v8)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>128897</th>\n",
       "      <td>0.980</td>\n",
       "      <td>49.759283</td>\n",
       "      <td>192.274230</td>\n",
       "      <td>35.870735</td>\n",
       "      <td>189.362492</td>\n",
       "      <td>-2.911739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189266</th>\n",
       "      <td>0.680</td>\n",
       "      <td>32.784902</td>\n",
       "      <td>191.872081</td>\n",
       "      <td>25.456094</td>\n",
       "      <td>189.592936</td>\n",
       "      <td>-2.279145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128524</th>\n",
       "      <td>1.990</td>\n",
       "      <td>89.237139</td>\n",
       "      <td>191.274526</td>\n",
       "      <td>58.038272</td>\n",
       "      <td>186.739582</td>\n",
       "      <td>-4.534945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81346</th>\n",
       "      <td>286.770</td>\n",
       "      <td>6.593781</td>\n",
       "      <td>191.009414</td>\n",
       "      <td>4.963803</td>\n",
       "      <td>193.194065</td>\n",
       "      <td>2.184651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189255</th>\n",
       "      <td>1.180</td>\n",
       "      <td>43.965555</td>\n",
       "      <td>189.544929</td>\n",
       "      <td>29.406008</td>\n",
       "      <td>184.568107</td>\n",
       "      <td>-4.976822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229126</th>\n",
       "      <td>2796.000</td>\n",
       "      <td>76.805319</td>\n",
       "      <td>189.305879</td>\n",
       "      <td>105.033939</td>\n",
       "      <td>185.517724</td>\n",
       "      <td>-3.788155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218412</th>\n",
       "      <td>600.590</td>\n",
       "      <td>18.021237</td>\n",
       "      <td>188.347294</td>\n",
       "      <td>21.057492</td>\n",
       "      <td>186.450525</td>\n",
       "      <td>-1.896768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86758</th>\n",
       "      <td>1.680</td>\n",
       "      <td>52.248101</td>\n",
       "      <td>187.538964</td>\n",
       "      <td>36.292188</td>\n",
       "      <td>182.302837</td>\n",
       "      <td>-5.236127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9697</th>\n",
       "      <td>177.510</td>\n",
       "      <td>5.934992</td>\n",
       "      <td>187.058809</td>\n",
       "      <td>6.926769</td>\n",
       "      <td>184.977466</td>\n",
       "      <td>-2.081343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188072</th>\n",
       "      <td>496.280</td>\n",
       "      <td>17.531823</td>\n",
       "      <td>186.351562</td>\n",
       "      <td>25.023422</td>\n",
       "      <td>180.799342</td>\n",
       "      <td>-5.552220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272013</th>\n",
       "      <td>1.390</td>\n",
       "      <td>36.635797</td>\n",
       "      <td>185.378347</td>\n",
       "      <td>21.542033</td>\n",
       "      <td>175.754439</td>\n",
       "      <td>-9.623908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175875</th>\n",
       "      <td>1.915</td>\n",
       "      <td>48.554676</td>\n",
       "      <td>184.822569</td>\n",
       "      <td>76.549599</td>\n",
       "      <td>190.237636</td>\n",
       "      <td>5.415066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253449</th>\n",
       "      <td>143.440</td>\n",
       "      <td>5.751983</td>\n",
       "      <td>184.578304</td>\n",
       "      <td>9.466444</td>\n",
       "      <td>175.235984</td>\n",
       "      <td>-9.342320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87913</th>\n",
       "      <td>2.990</td>\n",
       "      <td>74.030192</td>\n",
       "      <td>184.471605</td>\n",
       "      <td>43.808588</td>\n",
       "      <td>174.443673</td>\n",
       "      <td>-10.027931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234857</th>\n",
       "      <td>283.980</td>\n",
       "      <td>12.217561</td>\n",
       "      <td>183.500794</td>\n",
       "      <td>22.852937</td>\n",
       "      <td>170.207974</td>\n",
       "      <td>-13.292821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155327</th>\n",
       "      <td>328.600</td>\n",
       "      <td>14.282291</td>\n",
       "      <td>183.338550</td>\n",
       "      <td>14.316406</td>\n",
       "      <td>183.300412</td>\n",
       "      <td>-0.038137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281513</th>\n",
       "      <td>323.860</td>\n",
       "      <td>14.182251</td>\n",
       "      <td>183.218369</td>\n",
       "      <td>31.604282</td>\n",
       "      <td>164.436053</td>\n",
       "      <td>-18.782316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282458</th>\n",
       "      <td>0.630</td>\n",
       "      <td>14.310807</td>\n",
       "      <td>183.133441</td>\n",
       "      <td>12.748554</td>\n",
       "      <td>181.163883</td>\n",
       "      <td>-1.969558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163440</th>\n",
       "      <td>459.950</td>\n",
       "      <td>20.556690</td>\n",
       "      <td>182.887489</td>\n",
       "      <td>16.203856</td>\n",
       "      <td>186.387714</td>\n",
       "      <td>3.500226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246833</th>\n",
       "      <td>1.240</td>\n",
       "      <td>26.567590</td>\n",
       "      <td>182.163143</td>\n",
       "      <td>22.298485</td>\n",
       "      <td>178.928125</td>\n",
       "      <td>-3.235018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25679</th>\n",
       "      <td>0.530</td>\n",
       "      <td>11.222324</td>\n",
       "      <td>181.961015</td>\n",
       "      <td>10.736179</td>\n",
       "      <td>181.182618</td>\n",
       "      <td>-0.778397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49580</th>\n",
       "      <td>137.100</td>\n",
       "      <td>6.547261</td>\n",
       "      <td>181.768505</td>\n",
       "      <td>11.448274</td>\n",
       "      <td>169.172918</td>\n",
       "      <td>-12.595587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>0.990</td>\n",
       "      <td>19.892569</td>\n",
       "      <td>181.036816</td>\n",
       "      <td>11.607153</td>\n",
       "      <td>168.564326</td>\n",
       "      <td>-12.472490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173253</th>\n",
       "      <td>0.500</td>\n",
       "      <td>10.016552</td>\n",
       "      <td>180.982360</td>\n",
       "      <td>8.881491</td>\n",
       "      <td>178.681426</td>\n",
       "      <td>-2.300934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116922</th>\n",
       "      <td>1.835</td>\n",
       "      <td>35.541259</td>\n",
       "      <td>180.361865</td>\n",
       "      <td>23.011596</td>\n",
       "      <td>170.458730</td>\n",
       "      <td>-9.903135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25417</th>\n",
       "      <td>0.360</td>\n",
       "      <td>6.925589</td>\n",
       "      <td>180.234955</td>\n",
       "      <td>7.034020</td>\n",
       "      <td>180.524801</td>\n",
       "      <td>0.289847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219492</th>\n",
       "      <td>1.000</td>\n",
       "      <td>19.184206</td>\n",
       "      <td>180.182525</td>\n",
       "      <td>24.141234</td>\n",
       "      <td>184.089882</td>\n",
       "      <td>3.907357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37396</th>\n",
       "      <td>0.500</td>\n",
       "      <td>9.536207</td>\n",
       "      <td>180.072153</td>\n",
       "      <td>9.601462</td>\n",
       "      <td>180.200885</td>\n",
       "      <td>0.128733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284720</th>\n",
       "      <td>0.980</td>\n",
       "      <td>18.550541</td>\n",
       "      <td>179.928872</td>\n",
       "      <td>12.066772</td>\n",
       "      <td>169.954253</td>\n",
       "      <td>-9.974618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294859</th>\n",
       "      <td>173.960</td>\n",
       "      <td>9.232477</td>\n",
       "      <td>179.840925</td>\n",
       "      <td>9.910146</td>\n",
       "      <td>178.440989</td>\n",
       "      <td>-1.399936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134111</th>\n",
       "      <td>0.890</td>\n",
       "      <td>16.703596</td>\n",
       "      <td>179.765364</td>\n",
       "      <td>12.851295</td>\n",
       "      <td>174.092689</td>\n",
       "      <td>-5.672675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61203</th>\n",
       "      <td>0.930</td>\n",
       "      <td>17.397552</td>\n",
       "      <td>179.702690</td>\n",
       "      <td>20.792546</td>\n",
       "      <td>182.874935</td>\n",
       "      <td>3.172246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135068</th>\n",
       "      <td>143.300</td>\n",
       "      <td>7.708942</td>\n",
       "      <td>179.580170</td>\n",
       "      <td>10.099021</td>\n",
       "      <td>173.666008</td>\n",
       "      <td>-5.914162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164247</th>\n",
       "      <td>0.435</td>\n",
       "      <td>7.990817</td>\n",
       "      <td>179.349184</td>\n",
       "      <td>6.005117</td>\n",
       "      <td>172.981855</td>\n",
       "      <td>-6.367329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60720</th>\n",
       "      <td>319.890</td>\n",
       "      <td>17.668860</td>\n",
       "      <td>179.062780</td>\n",
       "      <td>25.488292</td>\n",
       "      <td>170.480725</td>\n",
       "      <td>-8.582056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27507</th>\n",
       "      <td>3.380</td>\n",
       "      <td>58.761442</td>\n",
       "      <td>178.243183</td>\n",
       "      <td>50.833819</td>\n",
       "      <td>175.061709</td>\n",
       "      <td>-3.181474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162838</th>\n",
       "      <td>0.980</td>\n",
       "      <td>16.921465</td>\n",
       "      <td>178.102350</td>\n",
       "      <td>17.516398</td>\n",
       "      <td>178.806684</td>\n",
       "      <td>0.704334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288925</th>\n",
       "      <td>182.510</td>\n",
       "      <td>10.774795</td>\n",
       "      <td>177.701722</td>\n",
       "      <td>14.148752</td>\n",
       "      <td>171.221719</td>\n",
       "      <td>-6.480003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200114</th>\n",
       "      <td>0.980</td>\n",
       "      <td>16.330148</td>\n",
       "      <td>177.354323</td>\n",
       "      <td>14.863153</td>\n",
       "      <td>175.257450</td>\n",
       "      <td>-2.096873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81130</th>\n",
       "      <td>413.990</td>\n",
       "      <td>25.048553</td>\n",
       "      <td>177.178721</td>\n",
       "      <td>35.855290</td>\n",
       "      <td>168.117670</td>\n",
       "      <td>-9.061051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13298</th>\n",
       "      <td>293.700</td>\n",
       "      <td>17.922839</td>\n",
       "      <td>176.994190</td>\n",
       "      <td>13.264742</td>\n",
       "      <td>182.714963</td>\n",
       "      <td>5.720773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31080</th>\n",
       "      <td>0.990</td>\n",
       "      <td>16.158567</td>\n",
       "      <td>176.907692</td>\n",
       "      <td>14.519378</td>\n",
       "      <td>174.467062</td>\n",
       "      <td>-2.440630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95629</th>\n",
       "      <td>148.990</td>\n",
       "      <td>9.262936</td>\n",
       "      <td>176.587010</td>\n",
       "      <td>12.438810</td>\n",
       "      <td>169.178216</td>\n",
       "      <td>-7.408794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297513</th>\n",
       "      <td>0.500</td>\n",
       "      <td>8.010069</td>\n",
       "      <td>176.498428</td>\n",
       "      <td>8.488835</td>\n",
       "      <td>177.750176</td>\n",
       "      <td>1.251749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83556</th>\n",
       "      <td>1.890</td>\n",
       "      <td>30.137774</td>\n",
       "      <td>176.395487</td>\n",
       "      <td>29.871883</td>\n",
       "      <td>176.197885</td>\n",
       "      <td>-0.197602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169054</th>\n",
       "      <td>107.490</td>\n",
       "      <td>6.870463</td>\n",
       "      <td>175.969096</td>\n",
       "      <td>6.130909</td>\n",
       "      <td>178.416263</td>\n",
       "      <td>2.447167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192184</th>\n",
       "      <td>349.950</td>\n",
       "      <td>22.993764</td>\n",
       "      <td>175.338090</td>\n",
       "      <td>25.419618</td>\n",
       "      <td>172.912439</td>\n",
       "      <td>-2.425651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263114</th>\n",
       "      <td>4.005</td>\n",
       "      <td>60.225229</td>\n",
       "      <td>175.058473</td>\n",
       "      <td>29.783080</td>\n",
       "      <td>152.586830</td>\n",
       "      <td>-22.471643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97051</th>\n",
       "      <td>0.890</td>\n",
       "      <td>13.043344</td>\n",
       "      <td>174.449780</td>\n",
       "      <td>13.766901</td>\n",
       "      <td>175.711100</td>\n",
       "      <td>1.261320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173319</th>\n",
       "      <td>0.945</td>\n",
       "      <td>13.796557</td>\n",
       "      <td>174.358203</td>\n",
       "      <td>9.712816</td>\n",
       "      <td>164.533071</td>\n",
       "      <td>-9.825132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              price    v8_pred    v8_error    v15_pred   v15_error  \\\n",
       "sample_id                                                            \n",
       "128897        0.980  49.759283  192.274230   35.870735  189.362492   \n",
       "189266        0.680  32.784902  191.872081   25.456094  189.592936   \n",
       "128524        1.990  89.237139  191.274526   58.038272  186.739582   \n",
       "81346       286.770   6.593781  191.009414    4.963803  193.194065   \n",
       "189255        1.180  43.965555  189.544929   29.406008  184.568107   \n",
       "229126     2796.000  76.805319  189.305879  105.033939  185.517724   \n",
       "218412      600.590  18.021237  188.347294   21.057492  186.450525   \n",
       "86758         1.680  52.248101  187.538964   36.292188  182.302837   \n",
       "9697        177.510   5.934992  187.058809    6.926769  184.977466   \n",
       "188072      496.280  17.531823  186.351562   25.023422  180.799342   \n",
       "272013        1.390  36.635797  185.378347   21.542033  175.754439   \n",
       "175875        1.915  48.554676  184.822569   76.549599  190.237636   \n",
       "253449      143.440   5.751983  184.578304    9.466444  175.235984   \n",
       "87913         2.990  74.030192  184.471605   43.808588  174.443673   \n",
       "234857      283.980  12.217561  183.500794   22.852937  170.207974   \n",
       "155327      328.600  14.282291  183.338550   14.316406  183.300412   \n",
       "281513      323.860  14.182251  183.218369   31.604282  164.436053   \n",
       "282458        0.630  14.310807  183.133441   12.748554  181.163883   \n",
       "163440      459.950  20.556690  182.887489   16.203856  186.387714   \n",
       "246833        1.240  26.567590  182.163143   22.298485  178.928125   \n",
       "25679         0.530  11.222324  181.961015   10.736179  181.182618   \n",
       "49580       137.100   6.547261  181.768505   11.448274  169.172918   \n",
       "5406          0.990  19.892569  181.036816   11.607153  168.564326   \n",
       "173253        0.500  10.016552  180.982360    8.881491  178.681426   \n",
       "116922        1.835  35.541259  180.361865   23.011596  170.458730   \n",
       "25417         0.360   6.925589  180.234955    7.034020  180.524801   \n",
       "219492        1.000  19.184206  180.182525   24.141234  184.089882   \n",
       "37396         0.500   9.536207  180.072153    9.601462  180.200885   \n",
       "284720        0.980  18.550541  179.928872   12.066772  169.954253   \n",
       "294859      173.960   9.232477  179.840925    9.910146  178.440989   \n",
       "134111        0.890  16.703596  179.765364   12.851295  174.092689   \n",
       "61203         0.930  17.397552  179.702690   20.792546  182.874935   \n",
       "135068      143.300   7.708942  179.580170   10.099021  173.666008   \n",
       "164247        0.435   7.990817  179.349184    6.005117  172.981855   \n",
       "60720       319.890  17.668860  179.062780   25.488292  170.480725   \n",
       "27507         3.380  58.761442  178.243183   50.833819  175.061709   \n",
       "162838        0.980  16.921465  178.102350   17.516398  178.806684   \n",
       "288925      182.510  10.774795  177.701722   14.148752  171.221719   \n",
       "200114        0.980  16.330148  177.354323   14.863153  175.257450   \n",
       "81130       413.990  25.048553  177.178721   35.855290  168.117670   \n",
       "13298       293.700  17.922839  176.994190   13.264742  182.714963   \n",
       "31080         0.990  16.158567  176.907692   14.519378  174.467062   \n",
       "95629       148.990   9.262936  176.587010   12.438810  169.178216   \n",
       "297513        0.500   8.010069  176.498428    8.488835  177.750176   \n",
       "83556         1.890  30.137774  176.395487   29.871883  176.197885   \n",
       "169054      107.490   6.870463  175.969096    6.130909  178.416263   \n",
       "192184      349.950  22.993764  175.338090   25.419618  172.912439   \n",
       "263114        4.005  60.225229  175.058473   29.783080  152.586830   \n",
       "97051         0.890  13.043344  174.449780   13.766901  175.711100   \n",
       "173319        0.945  13.796557  174.358203    9.712816  164.533071   \n",
       "\n",
       "           error_diff (v15 - v8)  \n",
       "sample_id                         \n",
       "128897                 -2.911739  \n",
       "189266                 -2.279145  \n",
       "128524                 -4.534945  \n",
       "81346                   2.184651  \n",
       "189255                 -4.976822  \n",
       "229126                 -3.788155  \n",
       "218412                 -1.896768  \n",
       "86758                  -5.236127  \n",
       "9697                   -2.081343  \n",
       "188072                 -5.552220  \n",
       "272013                 -9.623908  \n",
       "175875                  5.415066  \n",
       "253449                 -9.342320  \n",
       "87913                 -10.027931  \n",
       "234857                -13.292821  \n",
       "155327                 -0.038137  \n",
       "281513                -18.782316  \n",
       "282458                 -1.969558  \n",
       "163440                  3.500226  \n",
       "246833                 -3.235018  \n",
       "25679                  -0.778397  \n",
       "49580                 -12.595587  \n",
       "5406                  -12.472490  \n",
       "173253                 -2.300934  \n",
       "116922                 -9.903135  \n",
       "25417                   0.289847  \n",
       "219492                  3.907357  \n",
       "37396                   0.128733  \n",
       "284720                 -9.974618  \n",
       "294859                 -1.399936  \n",
       "134111                 -5.672675  \n",
       "61203                   3.172246  \n",
       "135068                 -5.914162  \n",
       "164247                 -6.367329  \n",
       "60720                  -8.582056  \n",
       "27507                  -3.181474  \n",
       "162838                  0.704334  \n",
       "288925                 -6.480003  \n",
       "200114                 -2.096873  \n",
       "81130                  -9.061051  \n",
       "13298                   5.720773  \n",
       "31080                  -2.440630  \n",
       "95629                  -7.408794  \n",
       "297513                  1.251749  \n",
       "83556                  -0.197602  \n",
       "169054                  2.447167  \n",
       "192184                 -2.425651  \n",
       "263114                -22.471643  \n",
       "97051                   1.261320  \n",
       "173319                 -9.825132  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary on the 50 Most Difficult Samples ---\n",
      "Number of times V15 (with images) improved the prediction: 38 / 50\n",
      "Number of times V15 (with images) worsened the prediction: 12 / 50\n",
      "Average change in SMAPE error after adding images: -4.2227 points\n",
      "\n",
      "Conclusion: On the most difficult predictions, adding images provided a slight improvement on average.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP & CONFIGURATION ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import hstack\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_DIR = 'input/'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 2. HELPER FUNCTIONS ---\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Calculates the overall SMAPE for a set of predictions.\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    return np.mean(numerator / denominator) * 100\n",
    "\n",
    "def individual_smape(y_true, y_pred):\n",
    "    \"\"\"Calculates the SMAPE for each individual prediction.\"\"\"\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    # Add a small epsilon to prevent division by zero for zero-priced items\n",
    "    return (numerator / (denominator + 1e-8)) * 100\n",
    "\n",
    "# --- 3. DATA LOADING & PREP ---\n",
    "print(\"--- Loading Data and Preparing Features ---\")\n",
    "try:\n",
    "    # Load all data, using 'sample_id' as the index for direct mapping\n",
    "    train_df_full = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'), index_col='sample_id')\n",
    "    test_df_full = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'), index_col='sample_id')\n",
    "    with open(os.path.join(INPUT_DIR, 'final_embeddings.pkl'), 'rb') as f:\n",
    "        image_embeddings_dict = pickle.load(f)\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading files: {e}. Please check your INPUT_DIR path.\")\n",
    "    exit()\n",
    "\n",
    "# Engineer features on the full training data\n",
    "train_df = train_df_full.dropna(subset=['catalog_content', 'price', 'image_link']).copy()\n",
    "train_df['log_price'] = np.log1p(train_df['price'])\n",
    "\n",
    "units = ['gb', 'oz', 'inch', 'mah', 'count', 'pack']\n",
    "for unit in units:\n",
    "    regex = r'(\\d+\\.?\\d*)\\s?' + re.escape(unit)\n",
    "    train_df[f'feat_{unit}'] = train_df['catalog_content'].str.extract(regex, flags=re.IGNORECASE).astype(float).fillna(0)\n",
    "\n",
    "# Split the data\n",
    "X_train_df, X_val_df = train_test_split(train_df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "y_train_log, y_val = X_train_df['log_price'], X_val_df['price']\n",
    "\n",
    "# Create Feature Extractors (TF-IDF and Scaler)\n",
    "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = [f'feat_{unit}' for unit in units]\n",
    "\n",
    "# --- 4. TRAIN V8 MODEL (TEXT-ONLY) ---\n",
    "print(\"\\n--- Training V8 Model (Text + Numericals only) ---\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_df['catalog_content'])\n",
    "X_train_num = scaler.fit_transform(X_train_df[numerical_cols])\n",
    "X_train_v8 = hstack([X_train_tfidf, X_train_num], format='csr')\n",
    "\n",
    "model_v8 = lgb.LGBMRegressor(random_state=RANDOM_STATE)\n",
    "model_v8.fit(X_train_v8, y_train_log)\n",
    "\n",
    "# --- 5. TRAIN V15 MODEL (TEXT + NUMERICALS + IMAGES) ---\n",
    "print(\"\\n--- Training V15 Model (Text + Numericals + Images) ---\")\n",
    "# Correctly determine the max sample_id from the entire dataset\n",
    "all_df = pd.concat([train_df_full, test_df_full])\n",
    "max_id = all_df.index.max()\n",
    "embedding_dim = 512 # From our CLIP model\n",
    "embedding_matrix = np.zeros((max_id + 1, embedding_dim), dtype=np.float32)\n",
    "\n",
    "# Populate the matrix using the full dataframe context\n",
    "for sample_id, row in tqdm(all_df.iterrows(), desc=\"Mapping Embeddings\"):\n",
    "    if row['image_link'] in image_embeddings_dict:\n",
    "        embedding = image_embeddings_dict.get(row['image_link'])\n",
    "        if embedding is not None:\n",
    "            embedding_matrix[sample_id] = embedding\n",
    "\n",
    "# Create the V15 training feature set by adding image embeddings\n",
    "X_train_img = embedding_matrix[X_train_df.index]\n",
    "X_train_v15 = hstack([X_train_v8, X_train_img], format='csr')\n",
    "\n",
    "model_v15 = lgb.LGBMRegressor(random_state=RANDOM_STATE)\n",
    "model_v15.fit(X_train_v15, y_train_log)\n",
    "\n",
    "# --- 6. PREPARE VALIDATION DATA & PREDICT ---\n",
    "print(\"\\n--- Generating Predictions for Both Models on Validation Set ---\")\n",
    "# Create V8 validation features\n",
    "X_val_tfidf = tfidf.transform(X_val_df['catalog_content'])\n",
    "X_val_num = scaler.transform(X_val_df[numerical_cols])\n",
    "X_val_v8 = hstack([X_val_tfidf, X_val_num], format='csr')\n",
    "\n",
    "# Create V15 validation features\n",
    "X_val_img = embedding_matrix[X_val_df.index]\n",
    "X_val_v15 = hstack([X_val_v8, X_val_img], format='csr')\n",
    "\n",
    "# Make predictions\n",
    "preds_v8_log = model_v8.predict(X_val_v8)\n",
    "preds_v15_log = model_v15.predict(X_val_v15)\n",
    "\n",
    "# Inverse transform predictions to original price scale\n",
    "preds_v8 = np.expm1(preds_v8_log)\n",
    "preds_v15 = np.expm1(preds_v15_log)\n",
    "\n",
    "# --- 7. FINAL DIAGNOSTIC: COMPARE WORST 50 PREDICTIONS ---\n",
    "print(\"\\n--- Diagnostic: Comparing Performance on V8's 50 Worst Predictions ---\")\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = X_val_df[['price', 'catalog_content']].copy()\n",
    "comparison_df['v8_pred'] = preds_v8\n",
    "comparison_df['v15_pred'] = preds_v15\n",
    "comparison_df['v8_error'] = individual_smape(comparison_df['price'], comparison_df['v8_pred'])\n",
    "comparison_df['v15_error'] = individual_smape(comparison_df['price'], comparison_df['v15_pred'])\n",
    "comparison_df['error_diff (v15 - v8)'] = comparison_df['v15_error'] - comparison_df['v8_error']\n",
    "\n",
    "# A negative 'error_diff' means V15 (with images) performed BETTER on that sample.\n",
    "# A positive 'error_diff' means V15 (with images) performed WORSE.\n",
    "\n",
    "# Get the 50 worst predictions from our champion V8 model\n",
    "worst_50_v8 = comparison_df.sort_values(by='v8_error', ascending=False).head(50)\n",
    "\n",
    "print(\"Analysis of the 50 samples where the V8 (text-only) model performed worst:\")\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "display(worst_50_v8[['price', 'v8_pred', 'v8_error', 'v15_pred', 'v15_error', 'error_diff (v15 - v8)']])\n",
    "\n",
    "# --- 8. QUANTITATIVE SUMMARY ---\n",
    "v15_improvements = (worst_50_v8['error_diff (v15 - v8)'] < 0).sum()\n",
    "v15_worsened = (worst_50_v8['error_diff (v15 - v8)'] > 0).sum()\n",
    "avg_error_diff = worst_50_v8['error_diff (v15 - v8)'].mean()\n",
    "\n",
    "print(f\"\\n--- Summary on the 50 Most Difficult Samples ---\")\n",
    "print(f\"Number of times V15 (with images) improved the prediction: {v15_improvements} / 50\")\n",
    "print(f\"Number of times V15 (with images) worsened the prediction: {v15_worsened} / 50\")\n",
    "print(f\"Average change in SMAPE error after adding images: {avg_error_diff:+.4f} points\")\n",
    "if avg_error_diff > 0:\n",
    "    print(\"\\nConclusion: On the most difficult predictions, adding images made the model worse on average.\")\n",
    "else:\n",
    "    print(\"\\nConclusion: On the most difficult predictions, adding images provided a slight improvement on average.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a8b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "price_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
